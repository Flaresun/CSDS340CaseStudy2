{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dd4a23-0743-4136-a4b4-c562830b55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc702a5-6ec4-41e1-9712-7fd70300ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accx = pd.read_csv(\"Downloads/Train_1/Acc_x_train_1.csv\", header=None)\n",
    "df_accy = pd.read_csv(\"Downloads/Train_1/Acc_y_train_1.csv\", header=None)\n",
    "df_accz = pd.read_csv(\"Downloads/Train_1/Acc_z_train_1.csv\", header=None)\n",
    "df_gyrx = pd.read_csv(\"Downloads/Train_1/Gyr_x_train_1.csv\", header=None)\n",
    "df_gyry = pd.read_csv(\"Downloads/Train_1/Gyr_y_train_1.csv\", header=None)\n",
    "df_gyrz = pd.read_csv(\"Downloads/Train_1/Gyr_z_train_1.csv\", header=None)\n",
    "labels = pd.read_csv(\"Downloads/Train_1/labels_train_1.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01dc58b-8f61-41f3-b765-d672958808aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.077889</td>\n",
       "      <td>-1.489613</td>\n",
       "      <td>-1.438783</td>\n",
       "      <td>-1.457909</td>\n",
       "      <td>-1.330404</td>\n",
       "      <td>-1.071001</td>\n",
       "      <td>-1.506137</td>\n",
       "      <td>-1.439139</td>\n",
       "      <td>-1.466377</td>\n",
       "      <td>-1.402347</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019677</td>\n",
       "      <td>-0.830715</td>\n",
       "      <td>-0.964348</td>\n",
       "      <td>-1.084696</td>\n",
       "      <td>-1.079483</td>\n",
       "      <td>-1.091208</td>\n",
       "      <td>-1.107158</td>\n",
       "      <td>-1.155510</td>\n",
       "      <td>-1.126861</td>\n",
       "      <td>-1.147167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.146887</td>\n",
       "      <td>-1.160566</td>\n",
       "      <td>-0.977457</td>\n",
       "      <td>-1.152031</td>\n",
       "      <td>-0.687245</td>\n",
       "      <td>-0.994804</td>\n",
       "      <td>-0.850746</td>\n",
       "      <td>-0.717160</td>\n",
       "      <td>-0.880832</td>\n",
       "      <td>-0.787904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939794</td>\n",
       "      <td>-0.681841</td>\n",
       "      <td>-0.672202</td>\n",
       "      <td>-0.806475</td>\n",
       "      <td>-0.852091</td>\n",
       "      <td>-0.756208</td>\n",
       "      <td>-0.291576</td>\n",
       "      <td>-0.888031</td>\n",
       "      <td>-0.839020</td>\n",
       "      <td>-0.808839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.810237</td>\n",
       "      <td>-0.803906</td>\n",
       "      <td>-0.937367</td>\n",
       "      <td>-0.888830</td>\n",
       "      <td>-0.853493</td>\n",
       "      <td>-0.866375</td>\n",
       "      <td>-1.002508</td>\n",
       "      <td>-0.936785</td>\n",
       "      <td>-0.911975</td>\n",
       "      <td>-0.909091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.818108</td>\n",
       "      <td>-0.805620</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.773826</td>\n",
       "      <td>-0.802690</td>\n",
       "      <td>-0.800057</td>\n",
       "      <td>-0.768268</td>\n",
       "      <td>-0.876734</td>\n",
       "      <td>-0.862862</td>\n",
       "      <td>-0.911913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.946278</td>\n",
       "      <td>-1.021568</td>\n",
       "      <td>-0.935827</td>\n",
       "      <td>-0.886008</td>\n",
       "      <td>-0.869594</td>\n",
       "      <td>-0.796406</td>\n",
       "      <td>-0.803121</td>\n",
       "      <td>-0.829610</td>\n",
       "      <td>-0.901657</td>\n",
       "      <td>-0.922945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.851171</td>\n",
       "      <td>-0.866475</td>\n",
       "      <td>-0.802323</td>\n",
       "      <td>-0.945886</td>\n",
       "      <td>-0.964929</td>\n",
       "      <td>-0.708072</td>\n",
       "      <td>0.112631</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.772995</td>\n",
       "      <td>-0.725184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.706771</td>\n",
       "      <td>-0.779546</td>\n",
       "      <td>-0.476212</td>\n",
       "      <td>-1.128938</td>\n",
       "      <td>-0.949800</td>\n",
       "      <td>-0.929042</td>\n",
       "      <td>-0.820955</td>\n",
       "      <td>-0.839328</td>\n",
       "      <td>-0.895739</td>\n",
       "      <td>-0.896472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.795637</td>\n",
       "      <td>-1.018215</td>\n",
       "      <td>-0.829386</td>\n",
       "      <td>-0.813845</td>\n",
       "      <td>-0.801443</td>\n",
       "      <td>-0.793584</td>\n",
       "      <td>-0.779280</td>\n",
       "      <td>-0.842876</td>\n",
       "      <td>-0.847628</td>\n",
       "      <td>-0.830808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>0.860861</td>\n",
       "      <td>0.926646</td>\n",
       "      <td>0.975781</td>\n",
       "      <td>1.000463</td>\n",
       "      <td>1.051148</td>\n",
       "      <td>1.099773</td>\n",
       "      <td>1.175936</td>\n",
       "      <td>1.191113</td>\n",
       "      <td>1.205718</td>\n",
       "      <td>1.119732</td>\n",
       "      <td>...</td>\n",
       "      <td>2.284014</td>\n",
       "      <td>2.307471</td>\n",
       "      <td>2.287213</td>\n",
       "      <td>2.286704</td>\n",
       "      <td>2.289199</td>\n",
       "      <td>2.289342</td>\n",
       "      <td>2.188936</td>\n",
       "      <td>2.147420</td>\n",
       "      <td>1.230552</td>\n",
       "      <td>1.223370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>0.867162</td>\n",
       "      <td>0.848416</td>\n",
       "      <td>0.827833</td>\n",
       "      <td>1.225863</td>\n",
       "      <td>1.845406</td>\n",
       "      <td>0.813404</td>\n",
       "      <td>1.207018</td>\n",
       "      <td>1.587939</td>\n",
       "      <td>1.426683</td>\n",
       "      <td>1.349285</td>\n",
       "      <td>...</td>\n",
       "      <td>2.523618</td>\n",
       "      <td>2.521772</td>\n",
       "      <td>2.532428</td>\n",
       "      <td>2.526269</td>\n",
       "      <td>2.517060</td>\n",
       "      <td>2.530315</td>\n",
       "      <td>2.525429</td>\n",
       "      <td>2.520982</td>\n",
       "      <td>2.525858</td>\n",
       "      <td>2.530050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>2.522300</td>\n",
       "      <td>2.519794</td>\n",
       "      <td>2.535938</td>\n",
       "      <td>2.519522</td>\n",
       "      <td>2.519759</td>\n",
       "      <td>2.509214</td>\n",
       "      <td>2.453885</td>\n",
       "      <td>2.368801</td>\n",
       "      <td>2.295425</td>\n",
       "      <td>2.289396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386323</td>\n",
       "      <td>0.382729</td>\n",
       "      <td>0.606361</td>\n",
       "      <td>0.699973</td>\n",
       "      <td>0.862712</td>\n",
       "      <td>1.504549</td>\n",
       "      <td>1.233929</td>\n",
       "      <td>1.075532</td>\n",
       "      <td>0.822507</td>\n",
       "      <td>0.852235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>0.982926</td>\n",
       "      <td>1.053619</td>\n",
       "      <td>0.964799</td>\n",
       "      <td>0.930685</td>\n",
       "      <td>1.123505</td>\n",
       "      <td>1.204031</td>\n",
       "      <td>1.053841</td>\n",
       "      <td>0.939975</td>\n",
       "      <td>0.678140</td>\n",
       "      <td>0.989545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923341</td>\n",
       "      <td>1.355667</td>\n",
       "      <td>1.886200</td>\n",
       "      <td>2.014653</td>\n",
       "      <td>1.942238</td>\n",
       "      <td>1.842240</td>\n",
       "      <td>1.638945</td>\n",
       "      <td>2.089226</td>\n",
       "      <td>2.309646</td>\n",
       "      <td>2.244656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>1.768710</td>\n",
       "      <td>0.953501</td>\n",
       "      <td>1.463969</td>\n",
       "      <td>1.813829</td>\n",
       "      <td>2.521835</td>\n",
       "      <td>1.923129</td>\n",
       "      <td>1.764970</td>\n",
       "      <td>1.641788</td>\n",
       "      <td>1.775368</td>\n",
       "      <td>1.678484</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.960735</td>\n",
       "      <td>-11.119735</td>\n",
       "      <td>-11.155887</td>\n",
       "      <td>-11.467462</td>\n",
       "      <td>-10.823796</td>\n",
       "      <td>-11.004394</td>\n",
       "      <td>-10.171395</td>\n",
       "      <td>-11.750076</td>\n",
       "      <td>-11.888035</td>\n",
       "      <td>-11.367570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -1.077889 -1.489613 -1.438783 -1.457909 -1.330404 -1.071001 -1.506137   \n",
       "1    -1.146887 -1.160566 -0.977457 -1.152031 -0.687245 -0.994804 -0.850746   \n",
       "2    -0.810237 -0.803906 -0.937367 -0.888830 -0.853493 -0.866375 -1.002508   \n",
       "3    -0.946278 -1.021568 -0.935827 -0.886008 -0.869594 -0.796406 -0.803121   \n",
       "4    -0.706771 -0.779546 -0.476212 -1.128938 -0.949800 -0.929042 -0.820955   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5206  0.860861  0.926646  0.975781  1.000463  1.051148  1.099773  1.175936   \n",
       "5207  0.867162  0.848416  0.827833  1.225863  1.845406  0.813404  1.207018   \n",
       "5208  2.522300  2.519794  2.535938  2.519522  2.519759  2.509214  2.453885   \n",
       "5209  0.982926  1.053619  0.964799  0.930685  1.123505  1.204031  1.053841   \n",
       "5210  1.768710  0.953501  1.463969  1.813829  2.521835  1.923129  1.764970   \n",
       "\n",
       "            7         8         9   ...         50         51         52  \\\n",
       "0    -1.439139 -1.466377 -1.402347  ...  -1.019677  -0.830715  -0.964348   \n",
       "1    -0.717160 -0.880832 -0.787904  ...  -0.939794  -0.681841  -0.672202   \n",
       "2    -0.936785 -0.911975 -0.909091  ...  -0.818108  -0.805620  -0.764217   \n",
       "3    -0.829610 -0.901657 -0.922945  ...  -0.851171  -0.866475  -0.802323   \n",
       "4    -0.839328 -0.895739 -0.896472  ...  -0.795637  -1.018215  -0.829386   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "5206  1.191113  1.205718  1.119732  ...   2.284014   2.307471   2.287213   \n",
       "5207  1.587939  1.426683  1.349285  ...   2.523618   2.521772   2.532428   \n",
       "5208  2.368801  2.295425  2.289396  ...  -0.386323   0.382729   0.606361   \n",
       "5209  0.939975  0.678140  0.989545  ...   0.923341   1.355667   1.886200   \n",
       "5210  1.641788  1.775368  1.678484  ... -10.960735 -11.119735 -11.155887   \n",
       "\n",
       "             53         54         55         56         57         58  \\\n",
       "0     -1.084696  -1.079483  -1.091208  -1.107158  -1.155510  -1.126861   \n",
       "1     -0.806475  -0.852091  -0.756208  -0.291576  -0.888031  -0.839020   \n",
       "2     -0.773826  -0.802690  -0.800057  -0.768268  -0.876734  -0.862862   \n",
       "3     -0.945886  -0.964929  -0.708072   0.112631  -0.709495  -0.772995   \n",
       "4     -0.813845  -0.801443  -0.793584  -0.779280  -0.842876  -0.847628   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "5206   2.286704   2.289199   2.289342   2.188936   2.147420   1.230552   \n",
       "5207   2.526269   2.517060   2.530315   2.525429   2.520982   2.525858   \n",
       "5208   0.699973   0.862712   1.504549   1.233929   1.075532   0.822507   \n",
       "5209   2.014653   1.942238   1.842240   1.638945   2.089226   2.309646   \n",
       "5210 -11.467462 -10.823796 -11.004394 -10.171395 -11.750076 -11.888035   \n",
       "\n",
       "             59  \n",
       "0     -1.147167  \n",
       "1     -0.808839  \n",
       "2     -0.911913  \n",
       "3     -0.725184  \n",
       "4     -0.830808  \n",
       "...         ...  \n",
       "5206   1.223370  \n",
       "5207   2.530050  \n",
       "5208   0.852235  \n",
       "5209   2.244656  \n",
       "5210 -11.367570  \n",
       "\n",
       "[5211 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ad47b1-ec92-423b-88f9-61da418a2b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.042689</td>\n",
       "      <td>-2.849762</td>\n",
       "      <td>-2.866504</td>\n",
       "      <td>-2.872480</td>\n",
       "      <td>-3.046087</td>\n",
       "      <td>-3.295942</td>\n",
       "      <td>-2.864503</td>\n",
       "      <td>-2.893296</td>\n",
       "      <td>-2.894975</td>\n",
       "      <td>-3.134266</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.704412</td>\n",
       "      <td>-2.635339</td>\n",
       "      <td>-2.640936</td>\n",
       "      <td>-2.724451</td>\n",
       "      <td>-2.703472</td>\n",
       "      <td>-2.688893</td>\n",
       "      <td>-2.698414</td>\n",
       "      <td>-2.699739</td>\n",
       "      <td>-2.695133</td>\n",
       "      <td>-2.704061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.695182</td>\n",
       "      <td>-2.651077</td>\n",
       "      <td>-2.985256</td>\n",
       "      <td>-3.222526</td>\n",
       "      <td>-3.320487</td>\n",
       "      <td>-2.823430</td>\n",
       "      <td>-2.846682</td>\n",
       "      <td>-2.807795</td>\n",
       "      <td>-2.881476</td>\n",
       "      <td>-2.836180</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.866929</td>\n",
       "      <td>-2.301257</td>\n",
       "      <td>-2.170395</td>\n",
       "      <td>-2.720272</td>\n",
       "      <td>-2.765513</td>\n",
       "      <td>-2.872680</td>\n",
       "      <td>-3.229364</td>\n",
       "      <td>-2.929117</td>\n",
       "      <td>-2.860563</td>\n",
       "      <td>-2.859279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.834726</td>\n",
       "      <td>-2.863199</td>\n",
       "      <td>-2.790414</td>\n",
       "      <td>-2.808154</td>\n",
       "      <td>-2.794856</td>\n",
       "      <td>-2.861103</td>\n",
       "      <td>-2.807620</td>\n",
       "      <td>-2.783585</td>\n",
       "      <td>-2.812906</td>\n",
       "      <td>-2.812025</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.825314</td>\n",
       "      <td>-2.802556</td>\n",
       "      <td>-2.800139</td>\n",
       "      <td>-2.793177</td>\n",
       "      <td>-2.791322</td>\n",
       "      <td>-2.794531</td>\n",
       "      <td>-2.793143</td>\n",
       "      <td>-2.820481</td>\n",
       "      <td>-2.775676</td>\n",
       "      <td>-2.795703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.800317</td>\n",
       "      <td>-2.810161</td>\n",
       "      <td>-2.784158</td>\n",
       "      <td>-2.803023</td>\n",
       "      <td>-2.790962</td>\n",
       "      <td>-2.791311</td>\n",
       "      <td>-2.797083</td>\n",
       "      <td>-2.778757</td>\n",
       "      <td>-2.772317</td>\n",
       "      <td>-2.761608</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.800582</td>\n",
       "      <td>-2.768778</td>\n",
       "      <td>-2.783484</td>\n",
       "      <td>-2.741648</td>\n",
       "      <td>-2.713543</td>\n",
       "      <td>-2.758237</td>\n",
       "      <td>-2.897527</td>\n",
       "      <td>-2.868514</td>\n",
       "      <td>-2.848029</td>\n",
       "      <td>-2.860777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.842639</td>\n",
       "      <td>-2.790093</td>\n",
       "      <td>-3.053917</td>\n",
       "      <td>-2.802748</td>\n",
       "      <td>-2.785534</td>\n",
       "      <td>-2.770120</td>\n",
       "      <td>-2.764651</td>\n",
       "      <td>-2.784589</td>\n",
       "      <td>-2.785037</td>\n",
       "      <td>-2.775467</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.853554</td>\n",
       "      <td>-2.466216</td>\n",
       "      <td>-2.635126</td>\n",
       "      <td>-2.616996</td>\n",
       "      <td>-2.615423</td>\n",
       "      <td>-2.613730</td>\n",
       "      <td>-2.631933</td>\n",
       "      <td>-2.621362</td>\n",
       "      <td>-2.623340</td>\n",
       "      <td>-2.658344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>4.862856</td>\n",
       "      <td>4.833499</td>\n",
       "      <td>4.696054</td>\n",
       "      <td>4.699973</td>\n",
       "      <td>4.773330</td>\n",
       "      <td>4.871981</td>\n",
       "      <td>5.052301</td>\n",
       "      <td>5.097763</td>\n",
       "      <td>4.987653</td>\n",
       "      <td>4.674787</td>\n",
       "      <td>...</td>\n",
       "      <td>5.579494</td>\n",
       "      <td>5.560665</td>\n",
       "      <td>5.584919</td>\n",
       "      <td>5.583107</td>\n",
       "      <td>5.584323</td>\n",
       "      <td>5.584497</td>\n",
       "      <td>5.471871</td>\n",
       "      <td>5.560278</td>\n",
       "      <td>5.248540</td>\n",
       "      <td>5.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>4.630720</td>\n",
       "      <td>4.541933</td>\n",
       "      <td>4.567318</td>\n",
       "      <td>4.485494</td>\n",
       "      <td>4.316146</td>\n",
       "      <td>4.522055</td>\n",
       "      <td>4.448091</td>\n",
       "      <td>4.353471</td>\n",
       "      <td>4.344239</td>\n",
       "      <td>4.478281</td>\n",
       "      <td>...</td>\n",
       "      <td>5.791800</td>\n",
       "      <td>5.784812</td>\n",
       "      <td>5.804014</td>\n",
       "      <td>5.787878</td>\n",
       "      <td>5.777855</td>\n",
       "      <td>5.793157</td>\n",
       "      <td>5.792883</td>\n",
       "      <td>5.797428</td>\n",
       "      <td>5.779568</td>\n",
       "      <td>5.800372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>5.798681</td>\n",
       "      <td>5.794846</td>\n",
       "      <td>5.806818</td>\n",
       "      <td>5.789714</td>\n",
       "      <td>5.786691</td>\n",
       "      <td>5.788064</td>\n",
       "      <td>5.751043</td>\n",
       "      <td>5.748245</td>\n",
       "      <td>5.787670</td>\n",
       "      <td>5.784572</td>\n",
       "      <td>...</td>\n",
       "      <td>7.085399</td>\n",
       "      <td>7.077344</td>\n",
       "      <td>6.440780</td>\n",
       "      <td>6.671841</td>\n",
       "      <td>6.272612</td>\n",
       "      <td>6.007163</td>\n",
       "      <td>5.931284</td>\n",
       "      <td>6.163937</td>\n",
       "      <td>6.469229</td>\n",
       "      <td>6.155357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>5.667661</td>\n",
       "      <td>5.600800</td>\n",
       "      <td>6.017774</td>\n",
       "      <td>6.642832</td>\n",
       "      <td>7.030796</td>\n",
       "      <td>6.161934</td>\n",
       "      <td>5.740471</td>\n",
       "      <td>5.891777</td>\n",
       "      <td>6.592513</td>\n",
       "      <td>6.658021</td>\n",
       "      <td>...</td>\n",
       "      <td>6.851487</td>\n",
       "      <td>6.824311</td>\n",
       "      <td>6.354765</td>\n",
       "      <td>5.720958</td>\n",
       "      <td>6.028545</td>\n",
       "      <td>6.837676</td>\n",
       "      <td>6.792713</td>\n",
       "      <td>6.397983</td>\n",
       "      <td>5.309706</td>\n",
       "      <td>5.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>6.732935</td>\n",
       "      <td>8.078105</td>\n",
       "      <td>7.976295</td>\n",
       "      <td>6.935153</td>\n",
       "      <td>5.353521</td>\n",
       "      <td>4.397137</td>\n",
       "      <td>5.786985</td>\n",
       "      <td>6.825773</td>\n",
       "      <td>6.021844</td>\n",
       "      <td>5.114242</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.020687</td>\n",
       "      <td>-0.440301</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>0.454290</td>\n",
       "      <td>0.128752</td>\n",
       "      <td>-1.409833</td>\n",
       "      <td>-0.665670</td>\n",
       "      <td>0.268034</td>\n",
       "      <td>-0.192508</td>\n",
       "      <td>0.062368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -3.042689 -2.849762 -2.866504 -2.872480 -3.046087 -3.295942 -2.864503   \n",
       "1    -2.695182 -2.651077 -2.985256 -3.222526 -3.320487 -2.823430 -2.846682   \n",
       "2    -2.834726 -2.863199 -2.790414 -2.808154 -2.794856 -2.861103 -2.807620   \n",
       "3    -2.800317 -2.810161 -2.784158 -2.803023 -2.790962 -2.791311 -2.797083   \n",
       "4    -2.842639 -2.790093 -3.053917 -2.802748 -2.785534 -2.770120 -2.764651   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5206  4.862856  4.833499  4.696054  4.699973  4.773330  4.871981  5.052301   \n",
       "5207  4.630720  4.541933  4.567318  4.485494  4.316146  4.522055  4.448091   \n",
       "5208  5.798681  5.794846  5.806818  5.789714  5.786691  5.788064  5.751043   \n",
       "5209  5.667661  5.600800  6.017774  6.642832  7.030796  6.161934  5.740471   \n",
       "5210  6.732935  8.078105  7.976295  6.935153  5.353521  4.397137  5.786985   \n",
       "\n",
       "            7         8         9   ...        50        51        52  \\\n",
       "0    -2.893296 -2.894975 -3.134266  ... -2.704412 -2.635339 -2.640936   \n",
       "1    -2.807795 -2.881476 -2.836180  ... -2.866929 -2.301257 -2.170395   \n",
       "2    -2.783585 -2.812906 -2.812025  ... -2.825314 -2.802556 -2.800139   \n",
       "3    -2.778757 -2.772317 -2.761608  ... -2.800582 -2.768778 -2.783484   \n",
       "4    -2.784589 -2.785037 -2.775467  ... -1.853554 -2.466216 -2.635126   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5206  5.097763  4.987653  4.674787  ...  5.579494  5.560665  5.584919   \n",
       "5207  4.353471  4.344239  4.478281  ...  5.791800  5.784812  5.804014   \n",
       "5208  5.748245  5.787670  5.784572  ...  7.085399  7.077344  6.440780   \n",
       "5209  5.891777  6.592513  6.658021  ...  6.851487  6.824311  6.354765   \n",
       "5210  6.825773  6.021844  5.114242  ... -1.020687 -0.440301  0.117684   \n",
       "\n",
       "            53        54        55        56        57        58        59  \n",
       "0    -2.724451 -2.703472 -2.688893 -2.698414 -2.699739 -2.695133 -2.704061  \n",
       "1    -2.720272 -2.765513 -2.872680 -3.229364 -2.929117 -2.860563 -2.859279  \n",
       "2    -2.793177 -2.791322 -2.794531 -2.793143 -2.820481 -2.775676 -2.795703  \n",
       "3    -2.741648 -2.713543 -2.758237 -2.897527 -2.868514 -2.848029 -2.860777  \n",
       "4    -2.616996 -2.615423 -2.613730 -2.631933 -2.621362 -2.623340 -2.658344  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5206  5.583107  5.584323  5.584497  5.471871  5.560278  5.248540  5.000719  \n",
       "5207  5.787878  5.777855  5.793157  5.792883  5.797428  5.779568  5.800372  \n",
       "5208  6.671841  6.272612  6.007163  5.931284  6.163937  6.469229  6.155357  \n",
       "5209  5.720958  6.028545  6.837676  6.792713  6.397983  5.309706  5.216553  \n",
       "5210  0.454290  0.128752 -1.409833 -0.665670  0.268034 -0.192508  0.062368  \n",
       "\n",
       "[5211 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873baade-a1f1-4c3d-8eec-6cb0cf0e5a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.490958</td>\n",
       "      <td>9.489082</td>\n",
       "      <td>9.491423</td>\n",
       "      <td>9.487663</td>\n",
       "      <td>9.456602</td>\n",
       "      <td>9.411505</td>\n",
       "      <td>9.475417</td>\n",
       "      <td>9.483038</td>\n",
       "      <td>9.474904</td>\n",
       "      <td>9.405075</td>\n",
       "      <td>...</td>\n",
       "      <td>9.599186</td>\n",
       "      <td>9.602288</td>\n",
       "      <td>9.607411</td>\n",
       "      <td>9.580112</td>\n",
       "      <td>9.577246</td>\n",
       "      <td>9.580301</td>\n",
       "      <td>9.577217</td>\n",
       "      <td>9.572769</td>\n",
       "      <td>9.575804</td>\n",
       "      <td>9.573681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.571114</td>\n",
       "      <td>9.572043</td>\n",
       "      <td>9.525989</td>\n",
       "      <td>9.419051</td>\n",
       "      <td>9.433341</td>\n",
       "      <td>9.547120</td>\n",
       "      <td>9.574582</td>\n",
       "      <td>9.579921</td>\n",
       "      <td>9.560588</td>\n",
       "      <td>9.562423</td>\n",
       "      <td>...</td>\n",
       "      <td>9.553980</td>\n",
       "      <td>9.720186</td>\n",
       "      <td>9.728905</td>\n",
       "      <td>9.603864</td>\n",
       "      <td>9.596016</td>\n",
       "      <td>9.564214</td>\n",
       "      <td>9.491248</td>\n",
       "      <td>9.527392</td>\n",
       "      <td>9.558546</td>\n",
       "      <td>9.564704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.572229</td>\n",
       "      <td>9.568761</td>\n",
       "      <td>9.545953</td>\n",
       "      <td>9.555552</td>\n",
       "      <td>9.566551</td>\n",
       "      <td>9.559498</td>\n",
       "      <td>9.552162</td>\n",
       "      <td>9.564638</td>\n",
       "      <td>9.567211</td>\n",
       "      <td>9.571621</td>\n",
       "      <td>...</td>\n",
       "      <td>9.578268</td>\n",
       "      <td>9.578344</td>\n",
       "      <td>9.583025</td>\n",
       "      <td>9.585149</td>\n",
       "      <td>9.583757</td>\n",
       "      <td>9.584450</td>\n",
       "      <td>9.581141</td>\n",
       "      <td>9.574511</td>\n",
       "      <td>9.573921</td>\n",
       "      <td>9.569919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.570084</td>\n",
       "      <td>9.565324</td>\n",
       "      <td>9.563295</td>\n",
       "      <td>9.567150</td>\n",
       "      <td>9.578377</td>\n",
       "      <td>9.585310</td>\n",
       "      <td>9.584341</td>\n",
       "      <td>9.575999</td>\n",
       "      <td>9.578835</td>\n",
       "      <td>9.575119</td>\n",
       "      <td>...</td>\n",
       "      <td>9.574114</td>\n",
       "      <td>9.579842</td>\n",
       "      <td>9.586607</td>\n",
       "      <td>9.590036</td>\n",
       "      <td>9.581470</td>\n",
       "      <td>9.608931</td>\n",
       "      <td>9.580922</td>\n",
       "      <td>9.568507</td>\n",
       "      <td>9.568925</td>\n",
       "      <td>9.568788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.582303</td>\n",
       "      <td>9.571570</td>\n",
       "      <td>9.545086</td>\n",
       "      <td>9.543188</td>\n",
       "      <td>9.571831</td>\n",
       "      <td>9.580899</td>\n",
       "      <td>9.578811</td>\n",
       "      <td>9.577550</td>\n",
       "      <td>9.580148</td>\n",
       "      <td>9.578360</td>\n",
       "      <td>...</td>\n",
       "      <td>9.807005</td>\n",
       "      <td>9.639631</td>\n",
       "      <td>9.626777</td>\n",
       "      <td>9.625316</td>\n",
       "      <td>9.628973</td>\n",
       "      <td>9.632085</td>\n",
       "      <td>9.628795</td>\n",
       "      <td>9.623408</td>\n",
       "      <td>9.619031</td>\n",
       "      <td>9.608914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>-8.282891</td>\n",
       "      <td>-8.396229</td>\n",
       "      <td>-8.389902</td>\n",
       "      <td>-8.387600</td>\n",
       "      <td>-8.348845</td>\n",
       "      <td>-8.214774</td>\n",
       "      <td>-8.307327</td>\n",
       "      <td>-7.994429</td>\n",
       "      <td>-8.185388</td>\n",
       "      <td>-8.392555</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.619747</td>\n",
       "      <td>-7.611326</td>\n",
       "      <td>-7.597790</td>\n",
       "      <td>-7.585320</td>\n",
       "      <td>-7.573957</td>\n",
       "      <td>-7.576759</td>\n",
       "      <td>-7.682734</td>\n",
       "      <td>-7.607888</td>\n",
       "      <td>-8.011768</td>\n",
       "      <td>-8.183101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>-8.463002</td>\n",
       "      <td>-8.499518</td>\n",
       "      <td>-8.480191</td>\n",
       "      <td>-8.488051</td>\n",
       "      <td>-8.543510</td>\n",
       "      <td>-8.499769</td>\n",
       "      <td>-8.509555</td>\n",
       "      <td>-8.531914</td>\n",
       "      <td>-8.560138</td>\n",
       "      <td>-8.486094</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.336627</td>\n",
       "      <td>-7.342599</td>\n",
       "      <td>-7.330303</td>\n",
       "      <td>-7.314101</td>\n",
       "      <td>-7.321386</td>\n",
       "      <td>-7.317653</td>\n",
       "      <td>-7.339694</td>\n",
       "      <td>-7.317185</td>\n",
       "      <td>-7.308462</td>\n",
       "      <td>-7.315197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>-7.350423</td>\n",
       "      <td>-7.321355</td>\n",
       "      <td>-7.321763</td>\n",
       "      <td>-7.318170</td>\n",
       "      <td>-7.361923</td>\n",
       "      <td>-7.338268</td>\n",
       "      <td>-7.385548</td>\n",
       "      <td>-7.419393</td>\n",
       "      <td>-7.439770</td>\n",
       "      <td>-7.412185</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.887199</td>\n",
       "      <td>-7.078360</td>\n",
       "      <td>-6.880469</td>\n",
       "      <td>-7.118758</td>\n",
       "      <td>-7.405318</td>\n",
       "      <td>-7.421931</td>\n",
       "      <td>-7.640936</td>\n",
       "      <td>-7.394678</td>\n",
       "      <td>-7.160276</td>\n",
       "      <td>-7.491745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>-7.901480</td>\n",
       "      <td>-7.932793</td>\n",
       "      <td>-7.578076</td>\n",
       "      <td>-7.004648</td>\n",
       "      <td>-6.543246</td>\n",
       "      <td>-7.326815</td>\n",
       "      <td>-7.732219</td>\n",
       "      <td>-7.524221</td>\n",
       "      <td>-7.246010</td>\n",
       "      <td>-6.912945</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.849268</td>\n",
       "      <td>-6.759649</td>\n",
       "      <td>-6.972498</td>\n",
       "      <td>-7.540784</td>\n",
       "      <td>-7.222463</td>\n",
       "      <td>-6.579629</td>\n",
       "      <td>-6.726451</td>\n",
       "      <td>-6.960347</td>\n",
       "      <td>-7.762013</td>\n",
       "      <td>-7.953816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>-6.802992</td>\n",
       "      <td>-5.918996</td>\n",
       "      <td>-5.746864</td>\n",
       "      <td>-6.678838</td>\n",
       "      <td>-7.677867</td>\n",
       "      <td>-8.543373</td>\n",
       "      <td>-7.589901</td>\n",
       "      <td>-6.589623</td>\n",
       "      <td>-7.153661</td>\n",
       "      <td>-8.053772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309613</td>\n",
       "      <td>0.695361</td>\n",
       "      <td>0.891638</td>\n",
       "      <td>0.063237</td>\n",
       "      <td>-0.354102</td>\n",
       "      <td>0.039655</td>\n",
       "      <td>0.924341</td>\n",
       "      <td>0.417735</td>\n",
       "      <td>0.391083</td>\n",
       "      <td>0.272630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     9.490958  9.489082  9.491423  9.487663  9.456602  9.411505  9.475417   \n",
       "1     9.571114  9.572043  9.525989  9.419051  9.433341  9.547120  9.574582   \n",
       "2     9.572229  9.568761  9.545953  9.555552  9.566551  9.559498  9.552162   \n",
       "3     9.570084  9.565324  9.563295  9.567150  9.578377  9.585310  9.584341   \n",
       "4     9.582303  9.571570  9.545086  9.543188  9.571831  9.580899  9.578811   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5206 -8.282891 -8.396229 -8.389902 -8.387600 -8.348845 -8.214774 -8.307327   \n",
       "5207 -8.463002 -8.499518 -8.480191 -8.488051 -8.543510 -8.499769 -8.509555   \n",
       "5208 -7.350423 -7.321355 -7.321763 -7.318170 -7.361923 -7.338268 -7.385548   \n",
       "5209 -7.901480 -7.932793 -7.578076 -7.004648 -6.543246 -7.326815 -7.732219   \n",
       "5210 -6.802992 -5.918996 -5.746864 -6.678838 -7.677867 -8.543373 -7.589901   \n",
       "\n",
       "            7         8         9   ...        50        51        52  \\\n",
       "0     9.483038  9.474904  9.405075  ...  9.599186  9.602288  9.607411   \n",
       "1     9.579921  9.560588  9.562423  ...  9.553980  9.720186  9.728905   \n",
       "2     9.564638  9.567211  9.571621  ...  9.578268  9.578344  9.583025   \n",
       "3     9.575999  9.578835  9.575119  ...  9.574114  9.579842  9.586607   \n",
       "4     9.577550  9.580148  9.578360  ...  9.807005  9.639631  9.626777   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5206 -7.994429 -8.185388 -8.392555  ... -7.619747 -7.611326 -7.597790   \n",
       "5207 -8.531914 -8.560138 -8.486094  ... -7.336627 -7.342599 -7.330303   \n",
       "5208 -7.419393 -7.439770 -7.412185  ... -6.887199 -7.078360 -6.880469   \n",
       "5209 -7.524221 -7.246010 -6.912945  ... -6.849268 -6.759649 -6.972498   \n",
       "5210 -6.589623 -7.153661 -8.053772  ...  0.309613  0.695361  0.891638   \n",
       "\n",
       "            53        54        55        56        57        58        59  \n",
       "0     9.580112  9.577246  9.580301  9.577217  9.572769  9.575804  9.573681  \n",
       "1     9.603864  9.596016  9.564214  9.491248  9.527392  9.558546  9.564704  \n",
       "2     9.585149  9.583757  9.584450  9.581141  9.574511  9.573921  9.569919  \n",
       "3     9.590036  9.581470  9.608931  9.580922  9.568507  9.568925  9.568788  \n",
       "4     9.625316  9.628973  9.632085  9.628795  9.623408  9.619031  9.608914  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5206 -7.585320 -7.573957 -7.576759 -7.682734 -7.607888 -8.011768 -8.183101  \n",
       "5207 -7.314101 -7.321386 -7.317653 -7.339694 -7.317185 -7.308462 -7.315197  \n",
       "5208 -7.118758 -7.405318 -7.421931 -7.640936 -7.394678 -7.160276 -7.491745  \n",
       "5209 -7.540784 -7.222463 -6.579629 -6.726451 -6.960347 -7.762013 -7.953816  \n",
       "5210  0.063237 -0.354102  0.039655  0.924341  0.417735  0.391083  0.272630  \n",
       "\n",
       "[5211 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd42012-cedd-434d-9c47-41f0d186b226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>-0.043541</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>-0.003196</td>\n",
       "      <td>-0.041717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.005284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005139</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>-0.039340</td>\n",
       "      <td>-0.011162</td>\n",
       "      <td>0.048962</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.118724</td>\n",
       "      <td>-0.091203</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.010531</td>\n",
       "      <td>-0.037979</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.008108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.006266</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.004173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008588</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009388</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.010726</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>-0.005366</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>-0.001131</td>\n",
       "      <td>0.016977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006262</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.009323</td>\n",
       "      <td>0.005414</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.007410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038316</td>\n",
       "      <td>-0.026126</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.006396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>-0.041205</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>0.008957</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.012234</td>\n",
       "      <td>0.020669</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.052558</td>\n",
       "      <td>0.049275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>0.036337</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.007272</td>\n",
       "      <td>0.010888</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.008544</td>\n",
       "      <td>0.008851</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.008513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.008586</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.037741</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.029058</td>\n",
       "      <td>0.033196</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>-0.009671</td>\n",
       "      <td>0.067382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>0.039663</td>\n",
       "      <td>0.031141</td>\n",
       "      <td>-0.046802</td>\n",
       "      <td>-0.086588</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.083629</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>-0.044531</td>\n",
       "      <td>0.029352</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>-0.004002</td>\n",
       "      <td>0.107119</td>\n",
       "      <td>0.024311</td>\n",
       "      <td>-0.099928</td>\n",
       "      <td>0.044294</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.111473</td>\n",
       "      <td>0.060230</td>\n",
       "      <td>-0.020810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>-0.165373</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.099343</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.117257</td>\n",
       "      <td>-0.047707</td>\n",
       "      <td>-0.142602</td>\n",
       "      <td>0.057541</td>\n",
       "      <td>0.163085</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.111454</td>\n",
       "      <td>0.130829</td>\n",
       "      <td>0.438029</td>\n",
       "      <td>0.128266</td>\n",
       "      <td>0.206361</td>\n",
       "      <td>-0.212983</td>\n",
       "      <td>-0.104296</td>\n",
       "      <td>0.289094</td>\n",
       "      <td>0.295490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.044689  0.001804  0.005565 -0.003941 -0.043541  0.064023  0.004858   \n",
       "1     0.005139 -0.004273 -0.039340 -0.011162  0.048962  0.020793  0.008060   \n",
       "2     0.006405  0.010463  0.005308  0.006266  0.004361  0.006804  0.008792   \n",
       "3     0.008588  0.006357  0.005966  0.006403  0.006500  0.006352  0.007036   \n",
       "4     0.006262 -0.006209  0.013502  0.012851  0.009323  0.005414  0.003975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5206  0.022355  0.019537  0.005410  0.013490  0.013013  0.010854 -0.041205   \n",
       "5207  0.036337  0.008847  0.007272  0.010888  0.007617  0.009544  0.009872   \n",
       "5208  0.008990  0.008754  0.008586  0.008532  0.008769  0.010709  0.012108   \n",
       "5209  0.039663  0.031141 -0.046802 -0.086588  0.033047  0.083629  0.022939   \n",
       "5210 -0.165373 -0.000014 -0.099343  0.014351  0.117257 -0.047707 -0.142602   \n",
       "\n",
       "            7         8         9   ...        50        51        52  \\\n",
       "0     0.005201 -0.003196 -0.041717  ...  0.011573  0.000616  0.008934   \n",
       "1     0.004712  0.006332  0.008611  ...  0.004502  0.118724 -0.091203   \n",
       "2     0.004064  0.006642  0.007317  ...  0.006895  0.007931  0.006392   \n",
       "3     0.005384  0.007965  0.004927  ...  0.009388  0.006430  0.010726   \n",
       "4     0.009056  0.005005  0.007410  ... -0.038316 -0.026126  0.010279   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5206  0.025523  0.015009  0.022133  ...  0.008812  0.008957  0.008602   \n",
       "5207  0.007857  0.009494 -0.001251  ...  0.009017  0.008902  0.008474   \n",
       "5208  0.007472  0.008090  0.008575  ...  0.042900  0.037741 -0.011432   \n",
       "5209 -0.044531  0.029352  0.017676  ...  0.001145 -0.004002  0.107119   \n",
       "5210  0.057541  0.163085  0.013052  ...  0.011567  0.111454  0.130829   \n",
       "\n",
       "            53        54        55        56        57        58        59  \n",
       "0     0.001367  0.004991  0.007416  0.004105  0.006542  0.007273  0.005284  \n",
       "1     0.001498  0.010531 -0.037979  0.013496  0.024111  0.008673  0.008108  \n",
       "2     0.005661  0.006388  0.006541  0.005314  0.005523  0.005919  0.004173  \n",
       "3     0.008252  0.006655 -0.005366  0.007891  0.003889 -0.001131  0.016977  \n",
       "4     0.006539  0.006541  0.005743  0.004882  0.007165  0.004564  0.006396  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5206  0.008387  0.008289  0.012234  0.020669  0.003951  0.052558  0.049275  \n",
       "5207  0.009110  0.008600  0.008544  0.008851  0.009113  0.008318  0.008513  \n",
       "5208  0.039146  0.029058  0.033196 -0.011753 -0.008387 -0.009671  0.067382  \n",
       "5209  0.024311 -0.099928  0.044294  0.003770  0.111473  0.060230 -0.020810  \n",
       "5210  0.438029  0.128266  0.206361 -0.212983 -0.104296  0.289094  0.295490  \n",
       "\n",
       "[5211 rows x 60 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gyrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ba2579-f22d-4631-8a48-bd550c60e30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.065891</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>-0.062646</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.001175</td>\n",
       "      <td>0.027014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>-0.021275</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>-0.001911</td>\n",
       "      <td>-0.003524</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>-0.001758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.057290</td>\n",
       "      <td>-0.008912</td>\n",
       "      <td>-0.048192</td>\n",
       "      <td>-0.038033</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>-0.024272</td>\n",
       "      <td>-0.024175</td>\n",
       "      <td>-0.007555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008158</td>\n",
       "      <td>-0.027742</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>-0.006638</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>-0.001650</td>\n",
       "      <td>-0.003087</td>\n",
       "      <td>-0.009248</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>-0.013746</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.014065</td>\n",
       "      <td>0.018043</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>-0.020283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>-0.013796</td>\n",
       "      <td>-0.018822</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009821</td>\n",
       "      <td>-0.007985</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.010818</td>\n",
       "      <td>-0.025211</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>0.019873</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>-0.016336</td>\n",
       "      <td>0.010107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001110</td>\n",
       "      <td>-0.007453</td>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.017854</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>-0.014810</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010408</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>-0.008621</td>\n",
       "      <td>-0.003543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>-0.002976</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>0.032933</td>\n",
       "      <td>0.019978</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>-0.025348</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.001660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>-0.002408</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>-0.001677</td>\n",
       "      <td>-0.001791</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>-0.003369</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.027336</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>-0.002831</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>-0.025632</td>\n",
       "      <td>-0.019468</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>-0.031496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>-0.045646</td>\n",
       "      <td>-0.018215</td>\n",
       "      <td>-0.005154</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.009414</td>\n",
       "      <td>-0.035906</td>\n",
       "      <td>-0.072077</td>\n",
       "      <td>-0.074924</td>\n",
       "      <td>-0.020582</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.062114</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>-0.067189</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.072693</td>\n",
       "      <td>0.071525</td>\n",
       "      <td>0.051334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>0.148066</td>\n",
       "      <td>0.344386</td>\n",
       "      <td>0.317019</td>\n",
       "      <td>0.203665</td>\n",
       "      <td>0.036971</td>\n",
       "      <td>-0.060184</td>\n",
       "      <td>-0.096810</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.051272</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332828</td>\n",
       "      <td>-0.088469</td>\n",
       "      <td>0.165899</td>\n",
       "      <td>0.079664</td>\n",
       "      <td>0.064408</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>-0.103241</td>\n",
       "      <td>-0.058597</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>-0.171866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.065891  0.001752  0.002214  0.008423  0.051032 -0.062646  0.000257   \n",
       "1     0.002253  0.008036  0.057290 -0.008912 -0.048192 -0.038033  0.021406   \n",
       "2     0.001315 -0.002543 -0.001650 -0.003087 -0.009248  0.011387  0.008010   \n",
       "3     0.013672  0.013287 -0.013796 -0.018822  0.001776  0.001189 -0.002629   \n",
       "4    -0.001110 -0.007453  0.010066  0.017854  0.002559 -0.004822 -0.014810   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5206  0.001933  0.007153  0.002779 -0.002976  0.003578  0.023481  0.021469   \n",
       "5207 -0.002408  0.002520  0.004120  0.007650  0.003421 -0.006914 -0.001677   \n",
       "5208  0.001726  0.001094  0.001068  0.001380  0.001159  0.002748  0.003701   \n",
       "5209 -0.045646 -0.018215 -0.005154  0.000623  0.009414 -0.035906 -0.072077   \n",
       "5210  0.148066  0.344386  0.317019  0.203665  0.036971 -0.060184 -0.096810   \n",
       "\n",
       "            7         8         9   ...        50        51        52  \\\n",
       "0     0.000201 -0.001175  0.027014  ...  0.008101 -0.021275  0.003528   \n",
       "1    -0.024272 -0.024175 -0.007555  ... -0.008158 -0.027742  0.015705   \n",
       "2    -0.008171 -0.013746  0.005458  ... -0.000102  0.006148 -0.005675   \n",
       "3     0.010648  0.010476 -0.010974  ... -0.009821 -0.007985  0.014600   \n",
       "4     0.001499  0.008106 -0.000204  ... -0.010408  0.001223  0.003416   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5206  0.032933  0.019978  0.000872  ...  0.001449  0.001165  0.001226   \n",
       "5207 -0.001791 -0.008862  0.002801  ...  0.001494  0.001179  0.001552   \n",
       "5208 -0.003369  0.001452  0.000204  ...  0.019313  0.027336 -0.005125   \n",
       "5209 -0.074924 -0.020582  0.011039  ...  0.001390  0.001864  0.062114   \n",
       "5210  0.005714  0.051272  0.008677  ... -0.332828 -0.088469  0.165899   \n",
       "\n",
       "            53        54        55        56        57        58        59  \n",
       "0     0.006000  0.003261 -0.001911 -0.003524  0.008354 -0.000773 -0.001758  \n",
       "1    -0.004679  0.004252 -0.006638  0.017289  0.004216  0.001229 -0.007947  \n",
       "2     0.001848  0.001094  0.000182 -0.014065  0.018043  0.007523 -0.020283  \n",
       "3     0.010818 -0.025211 -0.041516  0.019873  0.004239 -0.016336  0.010107  \n",
       "4    -0.000007 -0.001007  0.003715 -0.005493  0.003147 -0.008621 -0.003543  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5206  0.001371  0.001345  0.000857  0.017097 -0.025348 -0.001766  0.001660  \n",
       "5207  0.001185  0.001493  0.001522  0.001290  0.001443  0.001110  0.001402  \n",
       "5208 -0.002831  0.007357  0.010148 -0.025632 -0.019468 -0.003817 -0.031496  \n",
       "5209  0.020310 -0.067189  0.023217  0.012615  0.072693  0.071525  0.051334  \n",
       "5210  0.079664  0.064408 -0.088651 -0.103241 -0.058597  0.017539 -0.171866  \n",
       "\n",
       "[5211 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gyrz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c72aa98-8be0-4d72-b383-42ff4635c1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "...  ..\n",
       "5206  4\n",
       "5207  4\n",
       "5208  4\n",
       "5209  4\n",
       "5210  4\n",
       "\n",
       "[5211 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0ed12c-aa21-4918-ba9c-8c5dd67ae5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1857 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "163   4\n",
       "164   4\n",
       "165   4\n",
       "166   4\n",
       "167   4\n",
       "...  ..\n",
       "5206  4\n",
       "5207  4\n",
       "5208  4\n",
       "5209  4\n",
       "5210  4\n",
       "\n",
       "[1857 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.iloc[np.where(labels==4)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ec8f8c-a518-451c-a77c-9ef303a7d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = labels.iloc[np.where(labels==1)[0]]\n",
    "twos = labels.iloc[np.where(labels==2)[0]]\n",
    "threes = labels.iloc[np.where(labels==3)[0]]\n",
    "fours = labels.iloc[np.where(labels==4)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "521b6de9-020d-4fde-9de3-52b9130211a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Data Labeled 1: 0.29034734216081365 \n",
      "Percent of Data Labeled 2: 0.2807522548455191 \n",
      "Percent of Data Labeled 3: 0.07253886010362694 \n",
      "Percent of Data Labeled 4: 0.3563615428900403 \n",
      "CPU times: total: 266 ms\n",
      "Wall time: 327 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "plt.hist(labels, bins=30, edgecolor='black')\n",
    "plt.title('Feature Distribution (column 30)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "print(f\"Percent of Data Labeled 1: {len(ones) / len(labels)} \")\n",
    "print(f\"Percent of Data Labeled 2: {len(twos) / len(labels)} \")\n",
    "print(f\"Percent of Data Labeled 3: {len(threes) / len(labels)} \")\n",
    "print(f\"Percent of Data Labeled 4: {len(fours) / len(labels)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346afda5-2ffd-48c7-b96d-bb4bee7c92a4",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "### 6 datasets \n",
    "#### - 3 axes of linear acceleration \n",
    "#### - 3 axes of rotational acceleration. \n",
    "\n",
    "### Human Activity Recognition (HAR)\n",
    "\n",
    "#### Data is split into 1-minute time windows with no overlap. The i-th row represents the i-th minute time window. Each row has 60 values, corresponding to the sensor axis measurement at each second of the i-th minute. Finally, the label for the i-th time window is stored as the i-th entry of the labels file. \n",
    "\n",
    "#### Different row are sorted in increasing time order but they may not be consecutive minute after minute (normally, the next row would be a minute ahead of the previous row but that may not be the case here).\n",
    "\n",
    "#### We briefly note that the second data columns are a poor choice of features for statistical models. Could use deep-learning to automatically learn the feature representation. \n",
    "\n",
    "#### Do not use K-Fold Cross Validation due to the data being time dependent. Can use Holdout cross validation w/o shuffling (hold out the last k% of data or the middle k% of data)\n",
    "\n",
    "#### We also note that consecutive rows often denote consecutive time windows, meaning that the labels between those consecutive rows should not change too frequently. We could thus consider a smoothing of predictions.\n",
    "\n",
    "### Accuracy Metric\n",
    "#### Micro-averaged F1 score: Treats each example equally and favors more frequent classes (activities)\n",
    "\n",
    "#### Macro-averaged F1 score: treats each class equally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6274fff-2d38-471c-bb8f-84c6812b83bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff253445-2372-450c-903b-0a254515e968",
   "metadata": {},
   "source": [
    "### Notes from Hammerla-2016-Deep, convolutional, and recurre.pdf\n",
    "CNN -> RNN instead of just RNNo n sequence of individual samples \n",
    "Max-in norm regularization: After each mini-batch the incoming weights of each unit in the network are scaled to have a maximum euclidean length of d_in\n",
    "\n",
    "Mini-bathc with stratification (Can we train w stratification?)\n",
    "\n",
    "A forward LSTM contextualises the current time-step based on those it has seen previously and is inherently suitable for real-time application where, at inference time, the \"future\" is not yet known. Bi-directional LSTMs on the other hand use both the future and past context to intepret the input at tiemstep t, which makes then suitable for offline analysis scenarios\n",
    "\n",
    "For RNN: To prevent a large RNN from memorizing the entire input-output sequence and generalizing poorly, we can introduce \"breaks\" where the internal states of the RNN are reset to zero: After each mini-batch we decide to retain the internal state of the RNN with a carry-over probabilitty p_carry and reset it to zero otherwise.\n",
    "\n",
    "After each epoch of training we evaluate the performance of the model on the validation set. Each model is trained for at least 30 epochs and for a maximum of 300 epochs. After 30 epochs, training stops if there is no increase in validation performance for 10 subsequent epochs. We select the epoch that showed the best validation-set performance and apply the corresponding model to the test-set.\n",
    "\n",
    "fANOVA : Determines the extent to which each hyper-parameter cotributes to a network's performance. It builds a predictive model (random forest) of the model performance as a func- tion of the model’s hyper-parameters.This non-linear model is then decomposed into marginal and joint interaction func- tions of the hyper-parameters, from which the percentage contribution to overall variability of network performance is obtained.\n",
    "\n",
    "For CNNs we recommend to start exploring learning-rates, before optimis- ing the architecture of the network, as the learning-parameters had the largest effect on performance in our experiments\n",
    "\n",
    "However, we found that DNNs are very sensitive to their hyper-parameters and require a significant investment into parameter explo- ration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadba17e-8d2f-4be5-84ae-2dd1d1bf9390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db57684e-35fb-404a-bb2d-59198027f71d",
   "metadata": {},
   "source": [
    "### Two Approaches: \n",
    "#### 1) Feature Selection + Classifier \n",
    "#### 2) Neural Networks (DNNs, CNNs, RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f84375d-aed2-45fa-a9ae-e67e1a91f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import welch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f7632e-72ca-4f17-ac5f-c622c1f37c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(x):\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(x)\n",
    "    features['std'] = np.std(x)\n",
    "    features['var'] = np.var(x)\n",
    "    features['median'] = np.median(x)\n",
    "    features['min'] = np.min(x)\n",
    "    features['max'] = np.max(x)\n",
    "    features['range'] = np.max(x) - np.min(x)\n",
    "    features['p10'] = np.percentile(x, 10)\n",
    "    features['p25'] = np.percentile(x, 25)\n",
    "    features['p75'] = np.percentile(x, 75)\n",
    "    features['p90'] = np.percentile(x, 90)\n",
    "    features['iqr'] = features['p75'] - features['p25']\n",
    "    features['rms'] = np.sqrt(np.mean(x**2))\n",
    "    \n",
    "    # zero crossings\n",
    "    features['zero_crossings'] = np.sum(np.diff(np.sign(x)) != 0)\n",
    "\n",
    "    # peaks\n",
    "    peaks, _ = find_peaks(x)\n",
    "    features['num_peaks'] = len(peaks)\n",
    "\n",
    "    # shape statistics\n",
    "    features['skewness'] = skew(x)\n",
    "    features['kurtosis'] = kurtosis(x)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a3daff-035a-41a0-963e-06216de55b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_freq_features(x, fs=1.0):\n",
    "    # Compute FFT\n",
    "    freqs = np.fft.rfftfreq(len(x), 1/fs)\n",
    "    mag = np.abs(np.fft.rfft(x))\n",
    "\n",
    "    features = {}\n",
    "    features['spec_energy'] = np.sum(mag**2)\n",
    "    features['dom_freq'] = freqs[np.argmax(mag)]\n",
    "    features['mean_freq'] = np.sum(freqs * mag) / np.sum(mag)\n",
    "\n",
    "    # spectral entropy\n",
    "    p = mag / np.sum(mag)\n",
    "    features['spec_entropy'] = -np.sum(p * np.log2(p + 1e-12))\n",
    "\n",
    "    # spectral centroid\n",
    "    features['spec_centroid'] = np.sum(freqs * mag) / np.sum(mag)\n",
    "\n",
    "    # band powers (example)\n",
    "    features['low_freq_energy'] = np.sum(mag[(freqs <= 0.1)])\n",
    "    features['mid_freq_energy'] = np.sum(mag[(freqs > 0.1) & (freqs <= 0.3)])\n",
    "    features['high_freq_energy'] = np.sum(mag[(freqs > 0.3)])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165ac8de-c458-4ef8-b0ae-bc6a7dbab6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_autocorr_features(x):\n",
    "    corr = np.correlate(x - np.mean(x), x - np.mean(x), mode='full')\n",
    "    corr = corr[len(corr)//2:]  # keep positive lags\n",
    "\n",
    "    features = {\n",
    "        'autocorr_lag1': corr[1] / corr[0],\n",
    "        'autocorr_lag2': corr[2] / corr[0],\n",
    "        'autocorr_max': np.max(corr[1:]) / corr[0]\n",
    "    }\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac291c6-ac7f-44d9-8980-57c6e8089bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_axis(x):\n",
    "    f = {}\n",
    "    f.update(extract_time_features(x))\n",
    "    f.update(extract_freq_features(x))\n",
    "    f.update(extract_autocorr_features(x))\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e44c38b5-16da-4c98-936f-bb8fa1698587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(data):\n",
    "    rows = []\n",
    "    for i in range(data.shape[0]):\n",
    "        row_features = extract_features_from_axis(data.iloc[i])\n",
    "        rows.append(row_features)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2272322a-c48d-4bce-9ad7-a30d05c74d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5211, 28)\n",
      "CPU times: total: 19 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_df = extract_features_from_dataset(df_accx)\n",
    "print(features_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f12a82-4301-4686-bcf8-eba8cb5937a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>range</th>\n",
       "      <th>p10</th>\n",
       "      <th>p25</th>\n",
       "      <th>p75</th>\n",
       "      <th>...</th>\n",
       "      <th>dom_freq</th>\n",
       "      <th>mean_freq</th>\n",
       "      <th>spec_entropy</th>\n",
       "      <th>spec_centroid</th>\n",
       "      <th>low_freq_energy</th>\n",
       "      <th>mid_freq_energy</th>\n",
       "      <th>high_freq_energy</th>\n",
       "      <th>autocorr_lag1</th>\n",
       "      <th>autocorr_lag2</th>\n",
       "      <th>autocorr_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.190703</td>\n",
       "      <td>0.347513</td>\n",
       "      <td>0.120765</td>\n",
       "      <td>-1.137014</td>\n",
       "      <td>-1.883107</td>\n",
       "      <td>0.073880</td>\n",
       "      <td>1.956987</td>\n",
       "      <td>-1.518844</td>\n",
       "      <td>-1.456492</td>\n",
       "      <td>-0.996147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075244</td>\n",
       "      <td>2.953078</td>\n",
       "      <td>0.075244</td>\n",
       "      <td>96.776904</td>\n",
       "      <td>20.351286</td>\n",
       "      <td>11.425189</td>\n",
       "      <td>0.746275</td>\n",
       "      <td>0.575587</td>\n",
       "      <td>0.746275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.802572</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.027562</td>\n",
       "      <td>-0.821124</td>\n",
       "      <td>-1.160566</td>\n",
       "      <td>-0.291576</td>\n",
       "      <td>0.868990</td>\n",
       "      <td>-0.941571</td>\n",
       "      <td>-0.896220</td>\n",
       "      <td>-0.755923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>2.856027</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>59.682984</td>\n",
       "      <td>11.792792</td>\n",
       "      <td>9.541081</td>\n",
       "      <td>0.361170</td>\n",
       "      <td>0.161548</td>\n",
       "      <td>0.361170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.782922</td>\n",
       "      <td>0.151545</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>-0.803298</td>\n",
       "      <td>-1.002508</td>\n",
       "      <td>-0.089104</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>-0.927304</td>\n",
       "      <td>-0.867109</td>\n",
       "      <td>-0.741392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077634</td>\n",
       "      <td>2.751381</td>\n",
       "      <td>0.077634</td>\n",
       "      <td>57.537837</td>\n",
       "      <td>11.479539</td>\n",
       "      <td>7.405578</td>\n",
       "      <td>0.532272</td>\n",
       "      <td>0.292779</td>\n",
       "      <td>0.532272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844172</td>\n",
       "      <td>0.191887</td>\n",
       "      <td>0.036821</td>\n",
       "      <td>-0.875538</td>\n",
       "      <td>-1.061969</td>\n",
       "      <td>0.112631</td>\n",
       "      <td>1.174600</td>\n",
       "      <td>-0.989800</td>\n",
       "      <td>-0.949065</td>\n",
       "      <td>-0.796573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092494</td>\n",
       "      <td>3.046454</td>\n",
       "      <td>0.092494</td>\n",
       "      <td>62.094764</td>\n",
       "      <td>17.112121</td>\n",
       "      <td>10.828433</td>\n",
       "      <td>0.372950</td>\n",
       "      <td>0.052901</td>\n",
       "      <td>0.372950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.748877</td>\n",
       "      <td>0.238754</td>\n",
       "      <td>0.057004</td>\n",
       "      <td>-0.808516</td>\n",
       "      <td>-1.128938</td>\n",
       "      <td>0.088193</td>\n",
       "      <td>1.217131</td>\n",
       "      <td>-0.954825</td>\n",
       "      <td>-0.895922</td>\n",
       "      <td>-0.703322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106770</td>\n",
       "      <td>3.421837</td>\n",
       "      <td>0.106770</td>\n",
       "      <td>60.916198</td>\n",
       "      <td>20.515946</td>\n",
       "      <td>12.142907</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>0.119399</td>\n",
       "      <td>0.433426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>1.664175</td>\n",
       "      <td>0.742419</td>\n",
       "      <td>0.551187</td>\n",
       "      <td>2.168178</td>\n",
       "      <td>-0.017777</td>\n",
       "      <td>2.514385</td>\n",
       "      <td>2.532162</td>\n",
       "      <td>0.667173</td>\n",
       "      <td>1.051990</td>\n",
       "      <td>2.333478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062169</td>\n",
       "      <td>2.869605</td>\n",
       "      <td>0.062169</td>\n",
       "      <td>155.762443</td>\n",
       "      <td>20.055291</td>\n",
       "      <td>14.295202</td>\n",
       "      <td>0.909414</td>\n",
       "      <td>0.823551</td>\n",
       "      <td>0.909414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>1.929627</td>\n",
       "      <td>0.822055</td>\n",
       "      <td>0.675774</td>\n",
       "      <td>2.521377</td>\n",
       "      <td>-0.493841</td>\n",
       "      <td>2.649703</td>\n",
       "      <td>3.143544</td>\n",
       "      <td>0.846358</td>\n",
       "      <td>1.277756</td>\n",
       "      <td>2.534336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070662</td>\n",
       "      <td>3.077855</td>\n",
       "      <td>0.070662</td>\n",
       "      <td>183.029265</td>\n",
       "      <td>31.521028</td>\n",
       "      <td>17.435307</td>\n",
       "      <td>0.885083</td>\n",
       "      <td>0.773435</td>\n",
       "      <td>0.885083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>1.410571</td>\n",
       "      <td>0.658274</td>\n",
       "      <td>0.433325</td>\n",
       "      <td>1.282742</td>\n",
       "      <td>-0.386323</td>\n",
       "      <td>2.535938</td>\n",
       "      <td>2.922261</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.881660</td>\n",
       "      <td>2.015422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>3.376923</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>132.930171</td>\n",
       "      <td>36.100510</td>\n",
       "      <td>18.808526</td>\n",
       "      <td>0.834505</td>\n",
       "      <td>0.685026</td>\n",
       "      <td>0.834505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>1.320829</td>\n",
       "      <td>0.481604</td>\n",
       "      <td>0.231942</td>\n",
       "      <td>1.419496</td>\n",
       "      <td>0.100870</td>\n",
       "      <td>2.403774</td>\n",
       "      <td>2.302904</td>\n",
       "      <td>0.772698</td>\n",
       "      <td>0.987890</td>\n",
       "      <td>1.461541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078816</td>\n",
       "      <td>3.123163</td>\n",
       "      <td>0.078816</td>\n",
       "      <td>118.065433</td>\n",
       "      <td>22.733905</td>\n",
       "      <td>15.164752</td>\n",
       "      <td>0.807172</td>\n",
       "      <td>0.649683</td>\n",
       "      <td>0.807172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>-4.192443</td>\n",
       "      <td>6.269010</td>\n",
       "      <td>39.300481</td>\n",
       "      <td>0.618546</td>\n",
       "      <td>-11.888035</td>\n",
       "      <td>3.449188</td>\n",
       "      <td>15.337223</td>\n",
       "      <td>-11.369546</td>\n",
       "      <td>-11.001111</td>\n",
       "      <td>1.573687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>3.588459</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>679.321871</td>\n",
       "      <td>159.400176</td>\n",
       "      <td>89.251707</td>\n",
       "      <td>0.964993</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.964993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean       std        var    median        min       max      range  \\\n",
       "0    -1.190703  0.347513   0.120765 -1.137014  -1.883107  0.073880   1.956987   \n",
       "1    -0.802572  0.166017   0.027562 -0.821124  -1.160566 -0.291576   0.868990   \n",
       "2    -0.782922  0.151545   0.022966 -0.803298  -1.002508 -0.089104   0.913404   \n",
       "3    -0.844172  0.191887   0.036821 -0.875538  -1.061969  0.112631   1.174600   \n",
       "4    -0.748877  0.238754   0.057004 -0.808516  -1.128938  0.088193   1.217131   \n",
       "...        ...       ...        ...       ...        ...       ...        ...   \n",
       "5206  1.664175  0.742419   0.551187  2.168178  -0.017777  2.514385   2.532162   \n",
       "5207  1.929627  0.822055   0.675774  2.521377  -0.493841  2.649703   3.143544   \n",
       "5208  1.410571  0.658274   0.433325  1.282742  -0.386323  2.535938   2.922261   \n",
       "5209  1.320829  0.481604   0.231942  1.419496   0.100870  2.403774   2.302904   \n",
       "5210 -4.192443  6.269010  39.300481  0.618546 -11.888035  3.449188  15.337223   \n",
       "\n",
       "            p10        p25       p75  ...  dom_freq  mean_freq  spec_entropy  \\\n",
       "0     -1.518844  -1.456492 -0.996147  ...       0.0   0.075244      2.953078   \n",
       "1     -0.941571  -0.896220 -0.755923  ...       0.0   0.087039      2.856027   \n",
       "2     -0.927304  -0.867109 -0.741392  ...       0.0   0.077634      2.751381   \n",
       "3     -0.989800  -0.949065 -0.796573  ...       0.0   0.092494      3.046454   \n",
       "4     -0.954825  -0.895922 -0.703322  ...       0.0   0.106770      3.421837   \n",
       "...         ...        ...       ...  ...       ...        ...           ...   \n",
       "5206   0.667173   1.051990  2.333478  ...       0.0   0.062169      2.869605   \n",
       "5207   0.846358   1.277756  2.534336  ...       0.0   0.070662      3.077855   \n",
       "5208   0.735760   0.881660  2.015422  ...       0.0   0.087053      3.376923   \n",
       "5209   0.772698   0.987890  1.461541  ...       0.0   0.078816      3.123163   \n",
       "5210 -11.369546 -11.001111  1.573687  ...       0.0   0.089430      3.588459   \n",
       "\n",
       "      spec_centroid  low_freq_energy  mid_freq_energy  high_freq_energy  \\\n",
       "0          0.075244        96.776904        20.351286         11.425189   \n",
       "1          0.087039        59.682984        11.792792          9.541081   \n",
       "2          0.077634        57.537837        11.479539          7.405578   \n",
       "3          0.092494        62.094764        17.112121         10.828433   \n",
       "4          0.106770        60.916198        20.515946         12.142907   \n",
       "...             ...              ...              ...               ...   \n",
       "5206       0.062169       155.762443        20.055291         14.295202   \n",
       "5207       0.070662       183.029265        31.521028         17.435307   \n",
       "5208       0.087053       132.930171        36.100510         18.808526   \n",
       "5209       0.078816       118.065433        22.733905         15.164752   \n",
       "5210       0.089430       679.321871       159.400176         89.251707   \n",
       "\n",
       "      autocorr_lag1  autocorr_lag2  autocorr_max  \n",
       "0          0.746275       0.575587      0.746275  \n",
       "1          0.361170       0.161548      0.361170  \n",
       "2          0.532272       0.292779      0.532272  \n",
       "3          0.372950       0.052901      0.372950  \n",
       "4          0.433426       0.119399      0.433426  \n",
       "...             ...            ...           ...  \n",
       "5206       0.909414       0.823551      0.909414  \n",
       "5207       0.885083       0.773435      0.885083  \n",
       "5208       0.834505       0.685026      0.834505  \n",
       "5209       0.807172       0.649683      0.807172  \n",
       "5210       0.964993       0.919024      0.964993  \n",
       "\n",
       "[5211 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde37f8d-e01e-4d6f-95ce-f24480c53b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_axes = [df_accx,df_accy,df_accz,df_gyrx,df_gyry,df_gyrz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f81c06-2126-4d6f-907d-002b98a95c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5211, 168)\n",
      "CPU times: total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_dfs = []\n",
    "for axis_data in all_axes:   # list of 6 arrays, each shape (5211, 60)\n",
    "    feature_dfs.append(extract_features_from_dataset(axis_data))\n",
    "\n",
    "# concatenate side-by-side\n",
    "final_features = pd.concat(feature_dfs, axis=1)\n",
    "print(final_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18c85efa-0b51-4435-b094-d352feeddeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.190703</td>\n",
       "      <td>-2.842962</td>\n",
       "      <td>9.519042</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>-0.003501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.802572</td>\n",
       "      <td>-2.837896</td>\n",
       "      <td>9.566420</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>-0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.782922</td>\n",
       "      <td>-2.822640</td>\n",
       "      <td>9.573140</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>-0.001010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844172</td>\n",
       "      <td>-2.765951</td>\n",
       "      <td>9.582238</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>-0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.748877</td>\n",
       "      <td>-2.654359</td>\n",
       "      <td>9.614670</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>-0.001786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>1.664175</td>\n",
       "      <td>5.631538</td>\n",
       "      <td>-7.630455</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>1.929627</td>\n",
       "      <td>5.485373</td>\n",
       "      <td>-7.656154</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>1.410571</td>\n",
       "      <td>5.456462</td>\n",
       "      <td>-7.703096</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>-0.017097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>1.320829</td>\n",
       "      <td>6.489419</td>\n",
       "      <td>-7.022358</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.001385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>-4.192443</td>\n",
       "      <td>2.880316</td>\n",
       "      <td>-3.973912</td>\n",
       "      <td>0.053760</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>-0.020525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5211 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean      mean      mean      mean      mean      mean\n",
       "0    -1.190703 -2.842962  9.519042  0.006793  0.004836 -0.003501\n",
       "1    -0.802572 -2.837896  9.566420  0.005907  0.003179 -0.002039\n",
       "2    -0.782922 -2.822640  9.573140  0.006214  0.003801 -0.001010\n",
       "3    -0.844172 -2.765951  9.582238  0.006207  0.003457 -0.000866\n",
       "4    -0.748877 -2.654359  9.614670  0.006618  0.003826 -0.001786\n",
       "...        ...       ...       ...       ...       ...       ...\n",
       "5206  1.664175  5.631538 -7.630455  0.008872  0.003686  0.002382\n",
       "5207  1.929627  5.485373 -7.656154  0.006474  0.008250  0.001778\n",
       "5208  1.410571  5.456462 -7.703096  0.009686  0.013023 -0.017097\n",
       "5209  1.320829  6.489419 -7.022358  0.010631  0.006043  0.001385\n",
       "5210 -4.192443  2.880316 -3.973912  0.053760  0.080452 -0.020525\n",
       "\n",
       "[5211 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2d8a8-9d2a-4f90-a277-26d66d851ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6bb68bb-596a-441b-99d2-13b3a9ca680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "903edc0c-5eca-40e7-9d26-2275132cd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "VAL_FRAC = 0.20   # fraction of the data to hold out as validation (time-based, last VAL_FRAC)\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ====== Helper: time-aware train/val split ======\n",
    "def time_holdout_split(X, y, val_frac=VAL_FRAC):\n",
    "    \"\"\"\n",
    "    Hold out the last val_frac fraction of the samples as validation (no shuffling).\n",
    "    Inputs:\n",
    "      X: numpy array or DataFrame, shape (n_samples, n_features)\n",
    "      y: array-like shape (n_samples,)\n",
    "    Returns:\n",
    "      X_train, X_val, y_train, y_val  (as np.arrays)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    split = int(np.floor((1.0 - val_frac) * n))\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[:split].values\n",
    "        X_val = X.iloc[split:].values\n",
    "    else:\n",
    "        X_train = X[:split]\n",
    "        X_val = X[split:]\n",
    "    y_train = np.asarray(y[:split])\n",
    "    y_val = np.asarray(y[split:])\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def scale_data(X_train, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled, scaler\n",
    "\n",
    "def train_sklearn_model(clf, X_train, y_train, save_name=None):\n",
    "    clf.fit(X_train, y_train)\n",
    "    if save_name:\n",
    "        joblib.dump(clf, os.path.join(MODEL_DIR, save_name))\n",
    "    return clf\n",
    "\n",
    "def evaluate_model(clf, X_val, y_val, model_name):\n",
    "    y_pred = clf.predict(X_val)\n",
    "    micro = f1_score(y_val, y_pred, average=\"micro\")\n",
    "    macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"Micro F1: {micro:.4f} | Macro F1: {macro:.4f}\")\n",
    "    print(classification_report(y_val, y_pred, zero_division=0))\n",
    "    return {\"model\": model_name, \"micro_f1\": micro, \"macro_f1\": macro}\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128], n_classes=10, dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, n_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_torch_mlp(X_train, y_train, X_val, y_val, n_classes, device='cpu',\n",
    "                    hidden_dims=[256,128], lr=1e-3, batch_size=64, epochs=40, save_name=None):\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleMLP(input_dim=X_train.shape[1], hidden_dims=hidden_dims, n_classes=n_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_macro = -np.inf\n",
    "    best_state = None\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                out = model(xb)\n",
    "                preds = out.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(yb.numpy())\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        micro = f1_score(all_targets, all_preds, average='micro')\n",
    "        macro = f1_score(all_targets, all_preds, average='macro')\n",
    "        if macro > best_macro:\n",
    "            best_macro = macro\n",
    "            best_state = model.state_dict()\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] loss={avg_train_loss:.4f} microF1={micro:.4f} macroF1={macro:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    if save_name:\n",
    "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, save_name))\n",
    "    return model, {\"micro_f1\": micro, \"macro_f1\": macro}\n",
    "\n",
    "def run_all_models(X, y, val_frac=VAL_FRAC, random_state=RANDOM_STATE):\n",
    "    X_train, X_val, y_train, y_val = time_holdout_split(X, y, val_frac=val_frac)\n",
    "    print(f\"Train samples: {len(X_train)} | Val samples: {len(X_val)}\")\n",
    "\n",
    "    X_train_s, X_val_s, scaler = scale_data(X_train, X_val)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    lr = LogisticRegression(max_iter=2000, random_state=random_state, n_jobs=-1)\n",
    "    train_sklearn_model(lr, X_train_s, y_train, save_name=\"logistic_regression.joblib\")\n",
    "    results.append(evaluate_model(lr, X_val_s, y_val, \"LogisticRegression\"))\n",
    "\n",
    "    ridge = RidgeClassifier()\n",
    "    train_sklearn_model(ridge, X_train_s, y_train, save_name=\"ridge_classifier.joblib\")\n",
    "    results.append(evaluate_model(ridge, X_val_s, y_val, \"RidgeClassifier\"))\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1)\n",
    "    train_sklearn_model(rf, X_train_s, y_train, save_name=\"random_forest.joblib\")\n",
    "    results.append(evaluate_model(rf, X_val_s, y_val, \"RandomForest\"))\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(256,128), max_iter=500, early_stopping=True, random_state=random_state)\n",
    "    train_sklearn_model(mlp, X_train_s, y_train, save_name=\"sklearn_mlp.joblib\")\n",
    "    results.append(evaluate_model(mlp, X_val_s, y_val, \"Sklearn-MLP\"))\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric=\"mlogloss\", random_state=random_state)\n",
    "    xgb_clf.fit(X_train_s, y_train)\n",
    "    results.append(evaluate_model(xgb_clf, X_val_s, y_val, \"XGBoost\"))\n",
    "\n",
    "\n",
    "    lgb_clf = lgb.LGBMClassifier(n_estimators=200, random_state=random_state, verbose=-1)\n",
    "    lgb_clf.fit(X_train_s, y_train)\n",
    "    results.append(evaluate_model(lgb_clf, X_val_s, y_val, \"LightGBM\"))\n",
    "\n",
    "\n",
    "    cat_clf = CatBoostClassifier(verbose=0, random_state=random_state, iterations=500)\n",
    "    cat_clf.fit(X_train_s, y_train)\n",
    "    results.append(evaluate_model(cat_clf, X_val_s, y_val, \"CatBoost\"))\n",
    "\n",
    "\n",
    "    n_classes = int(np.max(y) + 1)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Training PyTorch MLP on device:\", device)\n",
    "    pt_model, pt_scores = train_torch_mlp(X_train_s, y_train, X_val_s, y_val, n_classes=n_classes, device=device,\n",
    "                                          hidden_dims=[256,128], lr=1e-3, batch_size=64, epochs=50, save_name=\"pt_mlp.pth\")\n",
    "    results.append({\"model\":\"PyTorch-MLP\", **pt_scores})\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"macro_f1\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc2863dd-70e1-4498-91cf-20733dd38bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5211, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2004d603-419e-4568-bedd-f0e597efa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_features\n",
    "y = np.array(labels-1).reshape(-1)# [0,1,2,3] instead of [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fe5ef9-a592-4f2c-a0e1-117eccbb8fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5211, 168), (5211,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea4b6a2d-efc8-469e-8514-c27ed68fd18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4168 | Val samples: 1043\n",
      "=== LogisticRegression ===\n",
      "Micro F1: 0.8245 | Macro F1: 0.8104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78       322\n",
      "           1       0.93      0.81      0.87       113\n",
      "           2       0.74      0.74      0.74        31\n",
      "           3       0.89      0.80      0.85       577\n",
      "\n",
      "    accuracy                           0.82      1043\n",
      "   macro avg       0.82      0.81      0.81      1043\n",
      "weighted avg       0.84      0.82      0.83      1043\n",
      "\n",
      "=== RidgeClassifier ===\n",
      "Micro F1: 0.8926 | Macro F1: 0.9117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       322\n",
      "           1       0.96      0.95      0.96       113\n",
      "           2       0.91      0.97      0.94        31\n",
      "           3       0.95      0.86      0.90       577\n",
      "\n",
      "    accuracy                           0.89      1043\n",
      "   macro avg       0.90      0.92      0.91      1043\n",
      "weighted avg       0.90      0.89      0.89      1043\n",
      "\n",
      "=== RandomForest ===\n",
      "Micro F1: 0.7843 | Macro F1: 0.8517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.88      0.72       322\n",
      "           1       0.96      0.97      0.96       113\n",
      "           2       0.91      0.97      0.94        31\n",
      "           3       0.92      0.68      0.78       577\n",
      "\n",
      "    accuracy                           0.78      1043\n",
      "   macro avg       0.85      0.88      0.85      1043\n",
      "weighted avg       0.83      0.78      0.79      1043\n",
      "\n",
      "=== Sklearn-MLP ===\n",
      "Micro F1: 0.8380 | Macro F1: 0.8457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.77       322\n",
      "           1       0.89      0.98      0.93       113\n",
      "           2       0.92      0.74      0.82        31\n",
      "           3       0.89      0.83      0.86       577\n",
      "\n",
      "    accuracy                           0.84      1043\n",
      "   macro avg       0.86      0.84      0.85      1043\n",
      "weighted avg       0.84      0.84      0.84      1043\n",
      "\n",
      "=== XGBoost ===\n",
      "Micro F1: 0.8284 | Macro F1: 0.8889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.89      0.77       322\n",
      "           1       0.95      1.00      0.97       113\n",
      "           2       1.00      0.97      0.98        31\n",
      "           3       0.93      0.75      0.83       577\n",
      "\n",
      "    accuracy                           0.83      1043\n",
      "   macro avg       0.89      0.90      0.89      1043\n",
      "weighted avg       0.85      0.83      0.83      1043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\ekkoi\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\ekkoi\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ekkoi\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ekkoi\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LightGBM ===\n",
      "Micro F1: 0.8313 | Macro F1: 0.8828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77       322\n",
      "           1       0.96      0.97      0.97       113\n",
      "           2       0.91      1.00      0.95        31\n",
      "           3       0.94      0.76      0.84       577\n",
      "\n",
      "    accuracy                           0.83      1043\n",
      "   macro avg       0.87      0.91      0.88      1043\n",
      "weighted avg       0.86      0.83      0.83      1043\n",
      "\n",
      "=== CatBoost ===\n",
      "Micro F1: 0.8686 | Macro F1: 0.9077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82       322\n",
      "           1       0.96      0.98      0.97       113\n",
      "           2       0.94      1.00      0.97        31\n",
      "           3       0.95      0.82      0.88       577\n",
      "\n",
      "    accuracy                           0.87      1043\n",
      "   macro avg       0.90      0.93      0.91      1043\n",
      "weighted avg       0.88      0.87      0.87      1043\n",
      "\n",
      "Training PyTorch MLP on device: cuda\n",
      "[Epoch 1/50] loss=0.3384 microF1=0.8811 macroF1=0.9049\n",
      "[Epoch 10/50] loss=0.0325 microF1=0.8351 macroF1=0.8760\n",
      "[Epoch 20/50] loss=0.0108 microF1=0.8293 macroF1=0.8146\n",
      "[Epoch 30/50] loss=0.0087 microF1=0.8610 macroF1=0.8983\n",
      "[Epoch 40/50] loss=0.0143 microF1=0.8629 macroF1=0.8979\n",
      "[Epoch 50/50] loss=0.0187 microF1=0.8658 macroF1=0.8662\n",
      "\n",
      "Summary (sorted by macro F1):\n",
      "                model  micro_f1  macro_f1\n",
      "0     RidgeClassifier  0.892617  0.911711\n",
      "1            CatBoost  0.868648  0.907654\n",
      "2             XGBoost  0.828380  0.888879\n",
      "3            LightGBM  0.831256  0.882784\n",
      "4         PyTorch-MLP  0.865772  0.866179\n",
      "5        RandomForest  0.784276  0.851666\n",
      "6         Sklearn-MLP  0.837967  0.845711\n",
      "7  LogisticRegression  0.824545  0.810373\n",
      "CPU times: total: 6min 35s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ====== Example usage ======\n",
    "# Expected: X: (n_samples, n_features), y: (n_samples,)\n",
    "\n",
    "results = run_all_models(X, y, val_frac=0.20)\n",
    "print(\"\\nSummary (sorted by macro F1):\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec4f17-d11b-43fd-98d5-05eb4581ef3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65277641-c0fb-4568-a73d-1b10ba72815e",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11123c4d-7e32-4f03-9ac2-519d68bbfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedd8b04-ed6f-4257-aba8-25a4e620e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([\n",
    "    df_accx,\n",
    "    df_accy,\n",
    "    df_accz,\n",
    "    df_gyrx,\n",
    "    df_gyry,\n",
    "    df_gyrz, \n",
    "], axis=1)   # shape = (N, 6, 60)\n",
    "\n",
    "y = np.array(labels-1).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bda50295-2c27-444d-baa8-633c8781fef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5211, 6, 60), (5211,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e8f49aa-ef99-4679-8479-0f9747d06f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "split = int(N * 0.8)\n",
    "\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afa4ca31-8f19-4d89-bff4-97afad431b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_flat = X_train.reshape(-1, 6*60)\n",
    "X_val_flat   = X_val.reshape(-1, 6*60)\n",
    "\n",
    "scaler.fit(X_train_flat)\n",
    "\n",
    "X_train = scaler.transform(X_train_flat).reshape(-1, 6, 60)\n",
    "X_val   = scaler.transform(X_val_flat).reshape(-1, 6, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d24ff0b-b25f-4cec-986c-7db428bdadc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4168, 6, 60), (1043, 6, 60))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42813804-2c58-4a1c-96bf-9f3cf7657c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5150aa8-55db-452e-aa48-0d88922a0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HARdataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = HARdataset(X_train, y_train)\n",
    "val_ds   = HARdataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a244b74-182a-4146-94b4-6cd5dfe9a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(6, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class HAR_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(6, 64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2),   # 60 -> 30\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),   # 30 -> 15\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.AdaptiveAvgPool1d(1)  # -> (batch, 256, 1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57d48d37-8272-49c0-b150-88a7bfe81f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-3):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        preds, true = [], []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                out = model(Xb)\n",
    "                pred = out.argmax(dim=1).cpu().numpy()\n",
    "                preds.append(pred)\n",
    "                true.append(yb.numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        true  = np.concatenate(true)\n",
    "\n",
    "        micro_f1 = f1_score(true, preds, average='micro')\n",
    "        macro_f1 = f1_score(true, preds, average='macro')\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | micro F1: {micro_f1:.4f}  macro F1: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c3b8a-6f02-4a9a-9df3-d11c7e98ab82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ed84623-5024-484a-82d8-b361629f92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | micro F1: 0.5542  macro F1: 0.6337\n",
      "Epoch 02 | micro F1: 0.6050  macro F1: 0.6970\n",
      "Epoch 03 | micro F1: 0.6443  macro F1: 0.7263\n",
      "Epoch 04 | micro F1: 0.6548  macro F1: 0.7379\n",
      "Epoch 05 | micro F1: 0.6357  macro F1: 0.7278\n",
      "Epoch 06 | micro F1: 0.5657  macro F1: 0.6889\n",
      "Epoch 07 | micro F1: 0.5638  macro F1: 0.6897\n",
      "Epoch 08 | micro F1: 0.5590  macro F1: 0.6914\n",
      "Epoch 09 | micro F1: 0.5743  macro F1: 0.7015\n",
      "Epoch 10 | micro F1: 0.5772  macro F1: 0.7036\n",
      "Epoch 11 | micro F1: 0.5532  macro F1: 0.6829\n",
      "Epoch 12 | micro F1: 0.5494  macro F1: 0.6811\n",
      "Epoch 13 | micro F1: 0.5743  macro F1: 0.6925\n",
      "Epoch 14 | micro F1: 0.5724  macro F1: 0.7025\n",
      "Epoch 15 | micro F1: 0.5676  macro F1: 0.6891\n",
      "Epoch 16 | micro F1: 0.5724  macro F1: 0.6854\n",
      "Epoch 17 | micro F1: 0.5896  macro F1: 0.7006\n",
      "Epoch 18 | micro F1: 0.6031  macro F1: 0.7077\n",
      "Epoch 19 | micro F1: 0.6242  macro F1: 0.7192\n",
      "Epoch 20 | micro F1: 0.6491  macro F1: 0.7341\n",
      "CPU times: total: 8.28 s\n",
      "Wall time: 9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_classes = len(np.unique(y))\n",
    "model = BaselineCNN(num_classes)\n",
    "train_model(model, train_loader, val_loader, epochs=20, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297f40a-54b5-43c3-b87d-fd8550a00501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b55b8a98-c604-4b04-af02-6796f1515b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | micro F1: 0.5216  macro F1: 0.4957\n",
      "Epoch 02 | micro F1: 0.5599  macro F1: 0.5959\n",
      "Epoch 03 | micro F1: 0.5580  macro F1: 0.5924\n",
      "Epoch 04 | micro F1: 0.5753  macro F1: 0.6200\n",
      "Epoch 05 | micro F1: 0.5877  macro F1: 0.6128\n",
      "Epoch 06 | micro F1: 0.5772  macro F1: 0.6378\n",
      "Epoch 07 | micro F1: 0.5475  macro F1: 0.6146\n",
      "Epoch 08 | micro F1: 0.5379  macro F1: 0.6509\n",
      "Epoch 09 | micro F1: 0.4938  macro F1: 0.5946\n",
      "Epoch 10 | micro F1: 0.4094  macro F1: 0.4824\n",
      "Epoch 11 | micro F1: 0.5101  macro F1: 0.6314\n",
      "Epoch 12 | micro F1: 0.4765  macro F1: 0.5915\n",
      "Epoch 13 | micro F1: 0.4803  macro F1: 0.6392\n",
      "Epoch 14 | micro F1: 0.4334  macro F1: 0.4267\n",
      "Epoch 15 | micro F1: 0.4458  macro F1: 0.4721\n",
      "Epoch 16 | micro F1: 0.5935  macro F1: 0.5079\n",
      "Epoch 17 | micro F1: 0.4640  macro F1: 0.5282\n",
      "Epoch 18 | micro F1: 0.4640  macro F1: 0.5747\n",
      "Epoch 19 | micro F1: 0.4257  macro F1: 0.5232\n",
      "Epoch 20 | micro F1: 0.4660  macro F1: 0.5066\n",
      "Epoch 21 | micro F1: 0.5590  macro F1: 0.6514\n",
      "Epoch 22 | micro F1: 0.4564  macro F1: 0.4324\n",
      "Epoch 23 | micro F1: 0.4506  macro F1: 0.5726\n",
      "Epoch 24 | micro F1: 0.5868  macro F1: 0.6568\n",
      "Epoch 25 | micro F1: 0.6107  macro F1: 0.6225\n",
      "CPU times: total: 19.6 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = HAR_CNN(num_classes)\n",
    "train_model(model, train_loader, val_loader, epochs=25, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d04c254f-e843-4d37-bca7-c983b9a5077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=6, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # We'll treat each time-step feature vector as size = input_channels\n",
    "        self.rnn = nn.LSTM(input_size=input_channels,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size * self.n_directions, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, seq_len) => convert to (batch, seq_len, channels)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        # RNN returns output: (batch, seq_len, hidden * num_directions)\n",
    "        out, (hn, cn) = self.rnn(x)  \n",
    "        # Use mean-pooling over time for robustness\n",
    "        pooled = out.mean(dim=1)  # (batch, hidden * num_directions)\n",
    "        return self.fc(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "195bafc4-ba6a-4acc-83b4-ccd78376cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=6, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=input_channels,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size * self.n_directions, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, seq_len) -> (batch, seq_len, channels)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        out, hn = self.rnn(x)\n",
    "        pooled = out.mean(dim=1)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb14c8d5-d0eb-421e-91e7-61921cf2385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create constant 'pe' matrix with values dependant on pos and i\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model) -> add pe[:, :seq_len, :]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfb62cdd-0b04-443f-a924-10830b9bbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=6, d_model=128, nhead=4, num_layers=3, dim_feedforward=256, dropout=0.2, max_len=60):\n",
    "        \"\"\"\n",
    "        - input_channels: number of sensor axes (6)\n",
    "        - d_model: embedding size for each time-step\n",
    "        - nhead: attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # project input channels -> d_model per time-step\n",
    "        self.input_proj = nn.Linear(input_channels, d_model)\n",
    "\n",
    "        # positional encoding and transformer encoder\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                   nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout,\n",
    "                                                   batch_first=True)  # batch_first=True for (batch, seq, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # we'll transpose to (batch, d_model, seq) before pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, seq_len) -> (batch, seq_len, channels)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        # project per time-step\n",
    "        x = self.input_proj(x)  # (batch, seq_len, d_model)\n",
    "        x = self.pos_enc(x)\n",
    "        # transformer encoder\n",
    "        x = self.transformer(x)  # (batch, seq_len, d_model)\n",
    "        # classifier expects (batch, d_model, seq_len)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de981ca5-f1fb-4916-8efc-f6e8eb728bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assume train_loader, val_loader already exist (from earlier)\n",
    "num_classes = len(np.unique(y))  # your label count\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48b16ff3-6588-4952-b1fb-eab6a6ff8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | micro F1: 0.6213  macro F1: 0.3485\n",
      "Epoch 02 | micro F1: 0.6203  macro F1: 0.4294\n",
      "Epoch 03 | micro F1: 0.6366  macro F1: 0.4756\n",
      "Epoch 04 | micro F1: 0.5599  macro F1: 0.4638\n",
      "Epoch 05 | micro F1: 0.6328  macro F1: 0.6526\n",
      "Epoch 06 | micro F1: 0.6107  macro F1: 0.5625\n",
      "Epoch 07 | micro F1: 0.7402  macro F1: 0.7665\n",
      "Epoch 08 | micro F1: 0.7057  macro F1: 0.7145\n",
      "Epoch 09 | micro F1: 0.6644  macro F1: 0.6914\n",
      "Epoch 10 | micro F1: 0.7402  macro F1: 0.7215\n",
      "Epoch 11 | micro F1: 0.7220  macro F1: 0.6931\n",
      "Epoch 12 | micro F1: 0.7651  macro F1: 0.7735\n",
      "Epoch 13 | micro F1: 0.7574  macro F1: 0.7638\n",
      "Epoch 14 | micro F1: 0.7756  macro F1: 0.7561\n",
      "Epoch 15 | micro F1: 0.7546  macro F1: 0.7526\n",
      "Epoch 16 | micro F1: 0.7574  macro F1: 0.7623\n",
      "Epoch 17 | micro F1: 0.7689  macro F1: 0.7605\n",
      "Epoch 18 | micro F1: 0.7747  macro F1: 0.7873\n",
      "Epoch 19 | micro F1: 0.7421  macro F1: 0.7624\n",
      "Epoch 20 | micro F1: 0.7047  macro F1: 0.6996\n",
      "Epoch 21 | micro F1: 0.6769  macro F1: 0.7091\n",
      "Epoch 22 | micro F1: 0.6817  macro F1: 0.6825\n",
      "Epoch 23 | micro F1: 0.6625  macro F1: 0.6557\n",
      "Epoch 24 | micro F1: 0.7565  macro F1: 0.7590\n",
      "Epoch 25 | micro F1: 0.7584  macro F1: 0.7580\n",
      "Epoch 26 | micro F1: 0.7833  macro F1: 0.7857\n",
      "Epoch 27 | micro F1: 0.7709  macro F1: 0.7680\n",
      "Epoch 28 | micro F1: 0.7728  macro F1: 0.7604\n",
      "Epoch 29 | micro F1: 0.7584  macro F1: 0.7562\n",
      "Epoch 30 | micro F1: 0.6826  macro F1: 0.7178\n",
      "CPU times: total: 37.9 s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm = LSTMModel(num_classes=num_classes)\n",
    "train_model(lstm, train_loader, val_loader, epochs=30, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "502459df-828d-42f0-b5fa-edc3a6c3aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | micro F1: 0.6616  macro F1: 0.3951\n",
      "Epoch 02 | micro F1: 0.6884  macro F1: 0.4864\n",
      "Epoch 03 | micro F1: 0.7124  macro F1: 0.7062\n",
      "Epoch 04 | micro F1: 0.6395  macro F1: 0.6674\n",
      "Epoch 05 | micro F1: 0.6012  macro F1: 0.6093\n",
      "Epoch 06 | micro F1: 0.6261  macro F1: 0.5829\n",
      "Epoch 07 | micro F1: 0.7325  macro F1: 0.7400\n",
      "Epoch 08 | micro F1: 0.6740  macro F1: 0.6789\n",
      "Epoch 09 | micro F1: 0.7584  macro F1: 0.7636\n",
      "Epoch 10 | micro F1: 0.7363  macro F1: 0.7433\n",
      "Epoch 11 | micro F1: 0.7555  macro F1: 0.7454\n",
      "Epoch 12 | micro F1: 0.7325  macro F1: 0.6987\n",
      "Epoch 13 | micro F1: 0.7315  macro F1: 0.7587\n",
      "Epoch 14 | micro F1: 0.6625  macro F1: 0.7080\n",
      "Epoch 15 | micro F1: 0.7172  macro F1: 0.7215\n",
      "Epoch 16 | micro F1: 0.7469  macro F1: 0.7635\n",
      "Epoch 17 | micro F1: 0.7402  macro F1: 0.7564\n",
      "Epoch 18 | micro F1: 0.7526  macro F1: 0.7820\n",
      "Epoch 19 | micro F1: 0.6318  macro F1: 0.7311\n",
      "Epoch 20 | micro F1: 0.6750  macro F1: 0.7422\n",
      "Epoch 21 | micro F1: 0.7220  macro F1: 0.7750\n",
      "Epoch 22 | micro F1: 0.6702  macro F1: 0.7480\n",
      "Epoch 23 | micro F1: 0.6673  macro F1: 0.7476\n",
      "Epoch 24 | micro F1: 0.6721  macro F1: 0.7396\n",
      "Epoch 25 | micro F1: 0.6337  macro F1: 0.7216\n",
      "Epoch 26 | micro F1: 0.6846  macro F1: 0.7688\n",
      "Epoch 27 | micro F1: 0.6568  macro F1: 0.7139\n",
      "Epoch 28 | micro F1: 0.6683  macro F1: 0.7199\n",
      "Epoch 29 | micro F1: 0.6711  macro F1: 0.7250\n",
      "Epoch 30 | micro F1: 0.6606  macro F1: 0.7143\n",
      "CPU times: total: 25.6 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gru = GRUModel(num_classes=num_classes)\n",
    "train_model(gru, train_loader, val_loader, epochs=30, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10009484-f0f4-4af0-98ea-7e0782ad4315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | micro F1: 0.6807  macro F1: 0.4832\n",
      "Epoch 02 | micro F1: 0.6472  macro F1: 0.4936\n",
      "Epoch 03 | micro F1: 0.7200  macro F1: 0.7679\n",
      "Epoch 04 | micro F1: 0.7076  macro F1: 0.7218\n",
      "Epoch 05 | micro F1: 0.7239  macro F1: 0.7562\n",
      "Epoch 06 | micro F1: 0.6232  macro F1: 0.6376\n",
      "Epoch 07 | micro F1: 0.6213  macro F1: 0.6400\n",
      "Epoch 08 | micro F1: 0.6136  macro F1: 0.6376\n",
      "Epoch 09 | micro F1: 0.7037  macro F1: 0.7345\n",
      "Epoch 10 | micro F1: 0.7133  macro F1: 0.6938\n",
      "Epoch 11 | micro F1: 0.7220  macro F1: 0.7824\n",
      "Epoch 12 | micro F1: 0.7718  macro F1: 0.8187\n",
      "Epoch 13 | micro F1: 0.7267  macro F1: 0.7621\n",
      "Epoch 14 | micro F1: 0.7450  macro F1: 0.7744\n",
      "Epoch 15 | micro F1: 0.7133  macro F1: 0.7229\n",
      "Epoch 16 | micro F1: 0.7258  macro F1: 0.7433\n",
      "Epoch 17 | micro F1: 0.6942  macro F1: 0.7571\n",
      "Epoch 18 | micro F1: 0.6433  macro F1: 0.7319\n",
      "Epoch 19 | micro F1: 0.6529  macro F1: 0.7132\n",
      "Epoch 20 | micro F1: 0.6155  macro F1: 0.6304\n",
      "Epoch 21 | micro F1: 0.6261  macro F1: 0.6768\n",
      "Epoch 22 | micro F1: 0.6165  macro F1: 0.6476\n",
      "Epoch 23 | micro F1: 0.6395  macro F1: 0.7009\n",
      "Epoch 24 | micro F1: 0.6261  macro F1: 0.6638\n",
      "Epoch 25 | micro F1: 0.6117  macro F1: 0.6146\n",
      "Epoch 26 | micro F1: 0.6222  macro F1: 0.6389\n",
      "Epoch 27 | micro F1: 0.6146  macro F1: 0.6541\n",
      "Epoch 28 | micro F1: 0.6318  macro F1: 0.6889\n",
      "Epoch 29 | micro F1: 0.6309  macro F1: 0.6772\n",
      "Epoch 30 | micro F1: 0.6280  macro F1: 0.6837\n",
      "CPU times: total: 47.2 s\n",
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer = TransformerModel(num_classes=num_classes, d_model=128, nhead=4, num_layers=3)\n",
    "train_model(transformer, train_loader, val_loader, epochs=30, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f227075-ea68-4a8c-85cc-893e6dab23ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82481fb7-33f0-4995-a2b7-fb2cc69f521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"Downloads/Train_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f70bc120-d85d-49fe-9c54-296d9854c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 score: 0.7939964685108888\n",
      "Macro-averaged F1 score: 0.7709647119816684\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Demo of logistic regression on mean and standard deviation of each sensor\n",
    "for activity recognition data\n",
    "@author: Kevin S. Xu\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "sensor_names = ['Acc_x', 'Acc_y', 'Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z']\n",
    "# Last row of training data for train/test split\n",
    "train_end_index = 3511\n",
    "# Logistic regression hyperparameters\n",
    "C = 1\n",
    "l1_ratio = 0.9\n",
    "max_iter = int(1e4)\n",
    "def predict_test(train_data, train_labels, test_data):\n",
    "    # Feature extraction: compute mean and standard deviation of each row for\n",
    "    # each sensor and concatenate across sensors to form the feature vector\n",
    "    mean_train_feature = np.mean(train_data, axis=1)\n",
    "    std_train_feature = np.std(train_data, axis=1)\n",
    "    train_features = np.hstack((mean_train_feature, std_train_feature))\n",
    "    mean_test_feature = np.mean(test_data, axis=1)\n",
    "    std_test_feature = np.std(test_data, axis=1)\n",
    "    test_features = np.hstack((mean_test_feature, std_test_feature))\n",
    "    # Standardize features and train a logistic regression model\n",
    "    scaler = StandardScaler()\n",
    "    train_features_std = scaler.fit_transform(train_features)\n",
    "    test_features_std = scaler.transform(test_features)\n",
    "    lr = LogisticRegression(penalty='elasticnet', solver='saga',\n",
    "    max_iter=max_iter, C=C, l1_ratio=l1_ratio)\n",
    "    lr.fit(train_features_std, train_labels)\n",
    "    test_outputs = lr.predict(test_features_std)\n",
    "    return test_outputs\n",
    "# Run this code only if being used as a script, not being imported\n",
    "if __name__ == \"__main__\":\n",
    "    # Load labels and training sensor data into 3-D array\n",
    "    labels = np.loadtxt('Downloads/Train_1/labels_train_1.csv', dtype='int')\n",
    "    data_slice_0 = np.loadtxt(BASE_PATH+sensor_names[0] + '_train_1.csv',\n",
    "    delimiter=',')\n",
    "    data = np.empty((data_slice_0.shape[0], data_slice_0.shape[1],\n",
    "    len(sensor_names)))\n",
    "    data[:, :, 0] = data_slice_0\n",
    "    del data_slice_0\n",
    "    for sensor_index in range(1, len(sensor_names)):\n",
    "        data[:, :, sensor_index] = np.loadtxt(BASE_PATH+\n",
    "        sensor_names[sensor_index] + '_train_1.csv', delimiter=',')\n",
    "    # Split into training and test by row index. Do not use a random split as\n",
    "    # rows are not independent!\n",
    "    train_data = data[:train_end_index+1, :, :]\n",
    "    train_labels = labels[:train_end_index+1]\n",
    "    test_data = data[train_end_index+1:, :, :]\n",
    "    test_labels = labels[train_end_index+1:]\n",
    "    test_outputs = predict_test(train_data, train_labels, test_data)\n",
    "    # Compute micro and macro-averaged F1 scores\n",
    "    micro_f1 = f1_score(test_labels, test_outputs, average='micro')\n",
    "    macro_f1 = f1_score(test_labels, test_outputs, average='macro')\n",
    "    print(f'Micro-averaged F1 score: {micro_f1}')\n",
    "    print(f'Macro-averaged F1 score: {macro_f1}')\n",
    "    # Examine outputs compared to labels\n",
    "    n_test = test_labels.size\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(np.arange(n_test), test_labels, 'b.')\n",
    "    plt.xlabel('Time window')\n",
    "    plt.ylabel('Target')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.arange(n_test), test_outputs, 'r.')\n",
    "    plt.xlabel('Time window')\n",
    "    plt.ylabel('Output (predicted target)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508bc14-1b73-479f-acc6-0a7589e2e06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69c045-6498-4630-968e-4abe19e52dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
